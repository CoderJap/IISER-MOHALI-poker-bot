{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: treys in c:\\users\\jasje\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install treys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from enum import Enum, auto\n",
    "\n",
    "class Action(Enum):\n",
    "    FOLD = 0\n",
    "    CHECK = 1\n",
    "    CALL = 2\n",
    "    RAISE = 3\n",
    "    BET = 4\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, id, stack):\n",
    "        self.id = id\n",
    "        self.stack = stack\n",
    "        self.hand = []\n",
    "        self.current_bet = 0\n",
    "        self.folded = False\n",
    "\n",
    "class TexasHoldEm:\n",
    "    def __init__(self, players, small_blind=10, big_blind=20):\n",
    "        self.players = players  # List of Player objects\n",
    "        self.small_blind = small_blind\n",
    "        self.big_blind = big_blind\n",
    "        self.deck = self.initialize_deck()\n",
    "        self.board = []  # Community cards\n",
    "        self.pot = 0\n",
    "        self.current_bet = 0\n",
    "        self.game_over = False\n",
    "        self.dealer_index = 0  # Index of the dealer in self.players\n",
    "        self.current_player_index = (self.dealer_index + 1) % len(self.players)  # Player to act\n",
    "        self.betting_round = 'pre-flop'  # Can be 'pre-flop', 'flop', 'turn', 'river'\n",
    "        self.round_bets = {}  # Tracks bets per player in the current round\n",
    "        self.last_raiser = None  # Tracks the last player who raised\n",
    "\n",
    "    def initialize_deck(self):\n",
    "        suits = ['H', 'D', 'C', 'S']\n",
    "        ranks = range(2, 15)  # 2-14 where 11-14 are J, Q, K, A\n",
    "        deck = [(rank, suit) for rank in ranks for suit in suits]\n",
    "        random.shuffle(deck)\n",
    "        return deck\n",
    "\n",
    "    def deal_hole_cards(self):\n",
    "        for player in self.players:\n",
    "            player.hand = [self.deck.pop(), self.deck.pop()]\n",
    "            player.current_bet = 0\n",
    "            player.folded = False\n",
    "        self.round_bets = {player.id: 0 for player in self.players}\n",
    "\n",
    "    def post_blinds(self):\n",
    "        small_blind_player = self.players[(self.dealer_index + 1) % len(self.players)]\n",
    "        big_blind_player = self.players[(self.dealer_index + 2) % len(self.players)]\n",
    "        \n",
    "        self._post_blind(small_blind_player, self.small_blind)\n",
    "        self._post_blind(big_blind_player, self.big_blind)\n",
    "        \n",
    "        self.current_bet = self.big_blind\n",
    "        self.last_raiser = big_blind_player.id\n",
    "\n",
    "        # Set current player to the one after the big blind\n",
    "        self.current_player_index = (self.dealer_index + 3) % len(self.players)\n",
    "\n",
    "    def _post_blind(self, player, amount):\n",
    "        player.stack -= amount\n",
    "        player.current_bet = amount\n",
    "        self.pot += amount\n",
    "        self.round_bets[player.id] = amount\n",
    "\n",
    "    def deal_flop(self):\n",
    "        self.deck.pop()  # Burn card\n",
    "        self.board.extend([self.deck.pop() for _ in range(3)])\n",
    "        self.betting_round = 'flop'\n",
    "        self.reset_bets()\n",
    "\n",
    "    def deal_turn(self):\n",
    "        self.deck.pop()  # Burn card\n",
    "        self.board.append(self.deck.pop())\n",
    "        self.betting_round = 'turn'\n",
    "        self.reset_bets()\n",
    "\n",
    "    def deal_river(self):\n",
    "        self.deck.pop()  # Burn card\n",
    "        self.board.append(self.deck.pop())\n",
    "        self.betting_round = 'river'\n",
    "        self.reset_bets()\n",
    "\n",
    "    def reset_bets(self):\n",
    "        self.current_bet = 0\n",
    "        for player in self.players:\n",
    "            player.current_bet = 0\n",
    "        self.round_bets = {player.id: 0 for player in self.players}\n",
    "        self.current_player_index = self.dealer_index  # Start with the player after the dealer\n",
    "        self.last_raiser = None\n",
    "\n",
    "    def get_current_player(self):\n",
    "        while True:\n",
    "            player = self.players[self.current_player_index]\n",
    "            if not player.folded:\n",
    "                return player\n",
    "            self.current_player_index = (self.current_player_index + 1) % len(self.players)\n",
    "\n",
    "    def get_available_actions(self, player):\n",
    "        if player.current_bet < self.current_bet:\n",
    "            # Player needs to call or fold\n",
    "            actions = [Action.FOLD, Action.CALL, Action.RAISE]\n",
    "        else:\n",
    "            # Player can check or bet/raise\n",
    "            if self.current_bet == 0:\n",
    "                actions = [Action.CHECK, Action.BET]\n",
    "            else:\n",
    "                actions = [Action.CHECK, Action.RAISE]\n",
    "        return actions\n",
    "\n",
    "    def execute_action(self, player, action, raise_amount=0):\n",
    "        if action == Action.FOLD:\n",
    "            self.handle_fold(player)\n",
    "        elif action == Action.CHECK:\n",
    "            self.handle_check(player)\n",
    "        elif action == Action.CALL:\n",
    "            self.handle_call(player)\n",
    "        elif action == Action.BET:\n",
    "            self.handle_bet(player, raise_amount)\n",
    "        elif action == Action.RAISE:\n",
    "            self.handle_raise(player, raise_amount)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        # Move to the next player\n",
    "        self.current_player_index = (self.current_player_index + 1) % len(self.players)\n",
    "\n",
    "    def handle_fold(self, player):\n",
    "        player.folded = True\n",
    "        print(f\"Player {player.id} folds.\")\n",
    "\n",
    "    def handle_check(self, player):\n",
    "        print(f\"Player {player.id} checks.\")\n",
    "\n",
    "    def handle_call(self, player):\n",
    "        call_amount = self.current_bet - player.current_bet\n",
    "        player.stack -= call_amount\n",
    "        player.current_bet += call_amount\n",
    "        self.pot += call_amount\n",
    "        self.round_bets[player.id] += call_amount\n",
    "        print(f\"Player {player.id} calls {call_amount}.\")\n",
    "\n",
    "    def handle_bet(self, player, amount):\n",
    "        if amount <= 0 or amount > player.stack:\n",
    "            raise ValueError(\"Invalid bet amount\")\n",
    "        player.stack -= amount\n",
    "        player.current_bet += amount\n",
    "        self.current_bet = player.current_bet\n",
    "        self.pot += amount\n",
    "        self.round_bets[player.id] += amount\n",
    "        self.last_raiser = player.id\n",
    "        print(f\"Player {player.id} bets {amount}.\")\n",
    "\n",
    "    def handle_raise(self, player, amount):\n",
    "        if amount <= 0 or amount > player.stack:\n",
    "            raise ValueError(\"Invalid raise amount\")\n",
    "        call_amount = self.current_bet - player.current_bet\n",
    "        total_amount = call_amount + amount\n",
    "        player.stack -= total_amount\n",
    "        player.current_bet += total_amount\n",
    "        self.current_bet = player.current_bet\n",
    "        self.pot += total_amount\n",
    "        self.round_bets[player.id] += total_amount\n",
    "        self.last_raiser = player.id\n",
    "        print(f\"Player {player.id} raises by {amount} to {player.current_bet}.\")\n",
    "\n",
    "    def is_round_over(self):\n",
    "        # The betting round is over when all players have either called the current bet or folded\n",
    "        active_players = [p for p in self.players if not p.folded]\n",
    "        if len(active_players) == 1:\n",
    "            return True  # Only one player remains\n",
    "        for player in active_players:\n",
    "            if player.id == self.last_raiser:\n",
    "                continue  # Skip the last raiser\n",
    "            if player.current_bet != self.current_bet:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def progress_round(self):\n",
    "        if self.betting_round == 'pre-flop':\n",
    "            self.deal_flop()\n",
    "        elif self.betting_round == 'flop':\n",
    "            self.deal_turn()\n",
    "        elif self.betting_round == 'turn':\n",
    "            self.deal_river()\n",
    "        elif self.betting_round == 'river':\n",
    "            self.game_over = True  # Proceed to showdown\n",
    "        else:\n",
    "            raise ValueError(\"Invalid betting round\")\n",
    "\n",
    "    def is_game_over(self):\n",
    "        # The game is over if only one player remains or all betting rounds are complete\n",
    "        active_players = [p for p in self.players if not p.folded]\n",
    "        if len(active_players) == 1:\n",
    "            return True\n",
    "        return self.game_over\n",
    "\n",
    "    def determine_winner(self):\n",
    "        # If only one player remains\n",
    "        active_players = [p for p in self.players if not p.folded]\n",
    "        if len(active_players) == 1:\n",
    "            winner = active_players[0]\n",
    "            winner.stack += self.pot\n",
    "            print(f\"Player {winner.id} wins the pot of {self.pot} by default.\")\n",
    "            self.pot = 0\n",
    "            return\n",
    "\n",
    "        # Showdown: compare hands\n",
    "        from treys import Evaluator, Card\n",
    "        evaluator = Evaluator()\n",
    "        best_rank = None\n",
    "        winners = []\n",
    "        for player in active_players:\n",
    "            hand = [Card.new(f\"{self.rank_to_str(rank)}{suit}\") for rank, suit in player.hand]\n",
    "            board = [Card.new(f\"{self.rank_to_str(rank)}{suit}\") for rank, suit in self.board]\n",
    "            rank = evaluator.evaluate(board, hand)\n",
    "            if best_rank is None or rank < best_rank:\n",
    "                best_rank = rank\n",
    "                winners = [player]\n",
    "            elif rank == best_rank:\n",
    "                winners.append(player)\n",
    "        # Split the pot among winners\n",
    "        split_pot = self.pot / len(winners)\n",
    "        for winner in winners:\n",
    "            winner.stack += split_pot\n",
    "            print(f\"Player {winner.id} wins {split_pot} from the pot.\")\n",
    "        self.pot = 0\n",
    "\n",
    "    def rank_to_str(self, rank):\n",
    "        if rank == 14:\n",
    "            return 'A'\n",
    "        elif rank == 13:\n",
    "            return 'K'\n",
    "        elif rank == 12:\n",
    "            return 'Q'\n",
    "        elif rank == 11:\n",
    "            return 'J'\n",
    "        elif rank == 10:\n",
    "            return 'T'\n",
    "        else:\n",
    "            return str(rank)\n",
    "\n",
    "    def get_reward(self, player):\n",
    "        # Define reward as the change in the player's stack\n",
    "        return player.stack - 1000  # Assuming initial stack is 1000\n",
    "\n",
    "    # Additional helper methods can be added as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeliefState:\n",
    "    def __init__(self, observed_actions, public_cards, pot_size=0):\n",
    "        self.observed_actions = observed_actions  # Sequence of actions taken\n",
    "        self.public_cards = public_cards          # Community cards revealed\n",
    "        self.private_cards = None                 # The AI's own hand\n",
    "        self.pot_size = pot_size                  # Pot size\n",
    "\n",
    "    def update(self, action, new_public_cards=None, pot_size=None):\n",
    "        self.observed_actions.append(action)\n",
    "        if new_public_cards is not None:\n",
    "            self.public_cards = new_public_cards\n",
    "        if pot_size is not None:\n",
    "            self.pot_size = pot_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)  # Output a scalar value\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_space)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.Softmax(dim=-1)(self.fc(x))\n",
    "\n",
    "\n",
    "\n",
    "MAX_FEATURE_LENGTH = 25  # Adjust this value based on your game dynamics\n",
    "\n",
    "def extract_features(belief_state):\n",
    "    # Convert actions to numerical representation\n",
    "    action_features = [action.value for action in belief_state.observed_actions]\n",
    "    \n",
    "    # Flatten public cards\n",
    "    public_card_features = [rank for (rank, suit) in belief_state.public_cards]\n",
    "    \n",
    "    # Combine features\n",
    "    features = action_features + public_card_features\n",
    "    \n",
    "    # Pad or truncate to fixed size\n",
    "    features = features[:MAX_FEATURE_LENGTH] + [0] * max(0, MAX_FEATURE_LENGTH - len(features))\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global dictionary to store CFRNodes\n",
    "cfr_nodes = {}\n",
    "\n",
    "class CFRNode:\n",
    "    def __init__(self, info_set, actions):\n",
    "        self.info_set = info_set\n",
    "        self.actions = actions\n",
    "        self.regret_sum = {action: 0.0 for action in actions}\n",
    "        self.strategy = {action: 1.0 / len(actions) for action in actions}\n",
    "        self.strategy_sum = {action: 0.0 for action in actions}\n",
    "\n",
    "    def get_strategy(self):\n",
    "        normalizing_sum = 0.0\n",
    "        for action in self.actions:\n",
    "            self.strategy[action] = max(self.regret_sum[action], 0.0)\n",
    "            normalizing_sum += self.strategy[action]\n",
    "        if normalizing_sum > 0:\n",
    "            for action in self.actions:\n",
    "                self.strategy[action] /= normalizing_sum\n",
    "        else:\n",
    "            for action in self.actions:\n",
    "                self.strategy[action] = 1.0 / len(self.actions)\n",
    "        # Update strategy sum for averaging\n",
    "        for action in self.actions:\n",
    "            self.strategy_sum[action] += self.strategy[action]\n",
    "        return self.strategy\n",
    "\n",
    "def get_or_create_cfr_node(info_set, actions):\n",
    "    if info_set not in cfr_nodes:\n",
    "        cfr_nodes[info_set] = CFRNode(info_set, actions)\n",
    "    return cfr_nodes[info_set]\n",
    "\n",
    "def cfr(node, p0, p1):\n",
    "    if node.is_terminal():\n",
    "        return node.utility()\n",
    "    info_set = node.get_info_set()\n",
    "    actions = node.get_available_actions()\n",
    "    cfr_node = get_or_create_cfr_node(info_set, actions)\n",
    "    strategy = cfr_node.get_strategy()\n",
    "    util = {}\n",
    "    node_util = 0\n",
    "    for action in actions:\n",
    "        next_node = node.take_action(action)\n",
    "        if node.current_player == 0:\n",
    "            util[action] = -cfr(next_node, p0 * strategy[action], p1)\n",
    "        else:\n",
    "            util[action] = -cfr(next_node, p0, p1 * strategy[action])\n",
    "        node_util += strategy[action] * util[action]\n",
    "    for action in actions:\n",
    "        regret = util[action] - node_util\n",
    "        if node.current_player == 0:\n",
    "            cfr_node.regret_sum[action] += p1 * regret\n",
    "        else:\n",
    "            cfr_node.regret_sum[action] += p0 * regret\n",
    "    return node_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_players = 2  # Since we're working with a two-player game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GameNode:\n",
    "#     def __init__(self, current_player, hands, board, history, num_players=2, \n",
    "#                  pot=0, player_stacks=None, player_bets=None, current_bet=0, \n",
    "#                  folded_players=None, betting_round='pre-flop', last_raiser=None):\n",
    "#         self.current_player = current_player  # ID of the current player\n",
    "#         self.hands = hands  # Dictionary mapping player IDs to their hands\n",
    "#         self.board = board  # Community cards\n",
    "#         self.history = history  # List of actions taken\n",
    "#         self.num_players = num_players  # Total number of players\n",
    "#         self.pot = pot  # Current size of the pot\n",
    "#         self.player_stacks = player_stacks if player_stacks is not None else {player_id: 1000 for player_id in hands}\n",
    "#         self.player_bets = player_bets if player_bets is not None else {player_id: 0 for player_id in hands}\n",
    "#         self.current_bet = current_bet  # Highest bet in the current betting round\n",
    "#         self.folded_players = folded_players if folded_players is not None else set()  # Players who have folded\n",
    "#         self.betting_round = betting_round  # Current betting round ('pre-flop', 'flop', 'turn', 'river')\n",
    "#         self.last_raiser = last_raiser  # ID of the last player who raised\n",
    "\n",
    "    \n",
    "#     def is_terminal(self):\n",
    "#         # Implement logic to determine if the game has ended\n",
    "#         pass\n",
    "    \n",
    "#     def utility(self):\n",
    "#         # Calculate the utility (payoff) for the current player\n",
    "#         pass\n",
    "    \n",
    "#     def get_info_set(self):\n",
    "#         # As defined earlier\n",
    "#         player_hand = self.hands[self.current_player]\n",
    "#         public_cards = self.board\n",
    "#         observed_actions = self.history\n",
    "#         info_set = (\n",
    "#             tuple(sorted(player_hand)),\n",
    "#             tuple(sorted(public_cards)),\n",
    "#             tuple(observed_actions)\n",
    "#         )\n",
    "#         return info_set\n",
    "    \n",
    "#     def get_available_actions(self):\n",
    "#         # Return a list of possible actions at this node\n",
    "#         return [Action.FOLD, Action.CALL, Action.RAISE]  # Example actions\n",
    "\n",
    "\n",
    "#     def take_action(self, action, amount=0):\n",
    "#         new_history = self.history + [action]\n",
    "#         new_hands = self.hands.copy()\n",
    "#         new_board = self.board.copy()\n",
    "#         new_pot = self.pot\n",
    "#         new_player_stacks = self.player_stacks.copy()\n",
    "#         new_player_bets = self.player_bets.copy()\n",
    "#         new_folded_players = self.folded_players.copy()\n",
    "#         new_current_bet = self.current_bet\n",
    "#         new_last_raiser = self.last_raiser\n",
    "#         new_betting_round = self.betting_round\n",
    "\n",
    "#         current_player_id = self.current_player\n",
    "\n",
    "#         if action == Action.FOLD:\n",
    "#             new_folded_players.add(current_player_id)\n",
    "#         elif action == Action.CALL:\n",
    "#             call_amount = new_current_bet - new_player_bets[current_player_id]\n",
    "#             if call_amount > new_player_stacks[current_player_id]:\n",
    "#                 call_amount = new_player_stacks[current_player_id]  # All-in\n",
    "#             new_player_stacks[current_player_id] -= call_amount\n",
    "#             new_player_bets[current_player_id] += call_amount\n",
    "#             new_pot += call_amount\n",
    "#         elif action == Action.RAISE:\n",
    "#             raise_amount = amount\n",
    "#             total_bet = new_current_bet + raise_amount\n",
    "#             bet_amount = total_bet - new_player_bets[current_player_id]\n",
    "#             if bet_amount > new_player_stacks[current_player_id]:\n",
    "#                 bet_amount = new_player_stacks[current_player_id]  # All-in\n",
    "#                 total_bet = new_player_bets[current_player_id] + bet_amount\n",
    "#             new_player_stacks[current_player_id] -= bet_amount\n",
    "#             new_player_bets[current_player_id] += bet_amount\n",
    "#             new_pot += bet_amount\n",
    "#             new_current_bet = total_bet\n",
    "#             new_last_raiser = current_player_id\n",
    "#         elif action == Action.CHECK:\n",
    "#             pass  # No changes needed for check\n",
    "#         elif action == Action.BET:\n",
    "#             bet_amount = amount\n",
    "#             if bet_amount > new_player_stacks[current_player_id]:\n",
    "#                 bet_amount = new_player_stacks[current_player_id]  # All-in\n",
    "#             new_player_stacks[current_player_id] -= bet_amount\n",
    "#             new_player_bets[current_player_id] += bet_amount\n",
    "#             new_pot += bet_amount\n",
    "#             new_current_bet = new_player_bets[current_player_id]\n",
    "#             new_last_raiser = current_player_id\n",
    "#         else:\n",
    "#             raise ValueError(\"Invalid action\")\n",
    "\n",
    "#         # Determine the next player\n",
    "#         next_player = (self.current_player + 1) % self.num_players\n",
    "#         while next_player in new_folded_players:\n",
    "#             next_player = (next_player + 1) % self.num_players\n",
    "#             if next_player == self.current_player:\n",
    "#                 break  # All other players have folded\n",
    "\n",
    "#         # Create a new GameNode with updated state\n",
    "#         new_node = GameNode(\n",
    "#             current_player=next_player,\n",
    "#             hands=new_hands,\n",
    "#             board=new_board,\n",
    "#             history=new_history,\n",
    "#             num_players=self.num_players,\n",
    "#             pot=new_pot,\n",
    "#             player_stacks=new_player_stacks,\n",
    "#             player_bets=new_player_bets,\n",
    "#             current_bet=new_current_bet,\n",
    "#             folded_players=new_folded_players,\n",
    "#             betting_round=new_betting_round,\n",
    "#             last_raiser=new_last_raiser\n",
    "#         )\n",
    "\n",
    "#         # Update betting round if needed\n",
    "#         if self.should_progress_round(new_node):\n",
    "#             new_node.progress_round()\n",
    "\n",
    "#         return new_node\n",
    "    \n",
    "#     def should_progress_round(self, node):\n",
    "#         active_players = [p for p in range(self.num_players) if p not in node.folded_players]\n",
    "#         # If only one player remains, the game ends\n",
    "#         if len(active_players) <= 1:\n",
    "#             return False\n",
    "#         # If all active players have matched the current bet or are all-in\n",
    "#         for player_id in active_players:\n",
    "#             player_bet = node.player_bets[player_id]\n",
    "#             if player_bet != node.current_bet and node.player_stacks[player_id] > 0:\n",
    "#                 return False\n",
    "#         return True\n",
    "    \n",
    "#     def progress_round(self):\n",
    "#         self.player_bets = {player_id: 0 for player_id in self.player_bets}\n",
    "#         self.current_bet = 0\n",
    "#         self.last_raiser = None\n",
    "\n",
    "#         if self.betting_round == 'pre-flop':\n",
    "#             self.deal_flop()\n",
    "#             self.betting_round = 'flop'\n",
    "#         elif self.betting_round == 'flop':\n",
    "#             self.deal_turn()\n",
    "#             self.betting_round = 'turn'\n",
    "#         elif self.betting_round == 'turn':\n",
    "#             self.deal_river()\n",
    "#             self.betting_round = 'river'\n",
    "#         elif self.betting_round == 'river':\n",
    "#             self.betting_round = 'showdown'\n",
    "#         else:\n",
    "#             pass  # Game is over\n",
    "\n",
    "#     def deal_flop(self):\n",
    "#         # Assuming self.deck is managed elsewhere\n",
    "#         self.board.extend([self.deck.pop() for _ in range(3)])\n",
    "\n",
    "#     def deal_turn(self):\n",
    "#         self.board.append(self.deck.pop())\n",
    "\n",
    "#     def deal_river(self):\n",
    "#         self.board.append(self.deck.pop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameNode:\n",
    "    def __init__(self, current_player, hands, board, history, num_players=2, \n",
    "                 pot=0, player_stacks=None, player_bets=None, current_bet=0, \n",
    "                 folded_players=None, betting_round='pre-flop', last_raiser=None, deck=None):\n",
    "        self.current_player = current_player  # ID of the current player\n",
    "        self.hands = hands  # Dictionary mapping player IDs to their hands\n",
    "        self.board = board  # Community cards\n",
    "        self.history = history  # List of actions taken\n",
    "        self.num_players = num_players  # Total number of players\n",
    "        self.pot = pot  # Current size of the pot\n",
    "        self.player_stacks = player_stacks if player_stacks is not None else {player_id: 1000 for player_id in hands}\n",
    "        self.player_bets = player_bets if player_bets is not None else {player_id: 0 for player_id in hands}\n",
    "        self.current_bet = current_bet  # Highest bet in the current betting round\n",
    "        self.folded_players = folded_players if folded_players is not None else set()  # Players who have folded\n",
    "        self.betting_round = betting_round  # Current betting round ('pre-flop', 'flop', 'turn', 'river', 'showdown')\n",
    "        self.last_raiser = last_raiser  # ID of the last player who raised\n",
    "        self.deck = deck if deck is not None else self.initialize_deck()  # Remaining cards in the deck\n",
    "\n",
    "    def initialize_deck(self):\n",
    "        suits = ['h', 'd', 'c', 's']\n",
    "        ranks = range(2, 15)  # 2-14 where 11-14 are J, Q, K, A\n",
    "        deck = [(rank, suit) for rank in ranks for suit in suits]\n",
    "        random.shuffle(deck)\n",
    "        return deck\n",
    "\n",
    "    def is_terminal(self):\n",
    "        active_players = [p for p in range(self.num_players) if p not in self.folded_players]\n",
    "        # Game ends if only one player remains or after showdown\n",
    "        if len(active_players) <= 1 or self.betting_round == 'showdown':\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def utility(self):\n",
    "        if not self.is_terminal():\n",
    "            return 0  # Utility is zero if the game is not over\n",
    "\n",
    "        active_players = [p for p in range(self.num_players) if p not in self.folded_players]\n",
    "        if len(active_players) == 1:\n",
    "            # Only one player left; they win the pot\n",
    "            winner = active_players[0]\n",
    "            if self.current_player == winner:\n",
    "                return self.pot\n",
    "            else:\n",
    "                return -self.pot\n",
    "        else:\n",
    "            # Showdown: compare hands\n",
    "            hand_strengths = {p: self.evaluate_hand(p) for p in active_players}\n",
    "            best_strength = min(hand_strengths.values())\n",
    "            winners = [p for p, strength in hand_strengths.items() if strength == best_strength]\n",
    "            if self.current_player in winners:\n",
    "                # Split the pot among winners\n",
    "                return self.pot / len(winners)\n",
    "            else:\n",
    "                return -self.pot / len(winners)\n",
    "\n",
    "    def evaluate_hand(self, player_id):\n",
    "        from treys import Evaluator, Card\n",
    "        evaluator = Evaluator()\n",
    "        player_hand = self.hands[player_id]\n",
    "        board = self.board\n",
    "        # Convert to treys Card objects\n",
    "        player_cards = [Card.new(f\"{self.rank_to_str(rank)}{suit}\") for rank, suit in player_hand]\n",
    "        board_cards = [Card.new(f\"{self.rank_to_str(rank)}{suit}\") for rank, suit in board]\n",
    "        rank = evaluator.evaluate(board_cards, player_cards)\n",
    "        return rank  # Lower rank means a better hand\n",
    "\n",
    "    def rank_to_str(self, rank):\n",
    "        rank_dict = {14: 'A', 13: 'K', 12: 'Q', 11: 'J', 10: 'T'}\n",
    "        return rank_dict.get(rank, str(rank))\n",
    "\n",
    "    def get_info_set(self):\n",
    "        player_hand = self.hands[self.current_player]\n",
    "        public_cards = self.board\n",
    "        observed_actions = self.history\n",
    "        info_set = (\n",
    "            tuple(sorted(player_hand)),\n",
    "            tuple(sorted(public_cards)),\n",
    "            tuple(observed_actions)\n",
    "        )\n",
    "        return info_set\n",
    "\n",
    "    def get_available_actions(self):\n",
    "        if self.current_bet > self.player_bets[self.current_player]:\n",
    "            # Player needs to call, raise, or fold\n",
    "            return [Action.FOLD, Action.CALL, Action.RAISE]\n",
    "        else:\n",
    "            # Player can check or bet/raise\n",
    "            if self.current_bet == 0:\n",
    "                return [Action.CHECK, Action.BET]\n",
    "            else:\n",
    "                return [Action.CHECK, Action.RAISE]\n",
    "\n",
    "    def take_action(self, action, amount=0):\n",
    "        new_history = self.history + [action]\n",
    "        new_hands = self.hands.copy()\n",
    "        new_board = self.board.copy()\n",
    "        new_pot = self.pot\n",
    "        new_player_stacks = self.player_stacks.copy()\n",
    "        new_player_bets = self.player_bets.copy()\n",
    "        new_folded_players = self.folded_players.copy()\n",
    "        new_current_bet = self.current_bet\n",
    "        new_last_raiser = self.last_raiser\n",
    "        new_betting_round = self.betting_round\n",
    "        new_deck = self.deck.copy()\n",
    "\n",
    "        current_player_id = self.current_player\n",
    "\n",
    "        if action == Action.FOLD:\n",
    "            new_folded_players.add(current_player_id)\n",
    "        elif action == Action.CALL:\n",
    "            call_amount = new_current_bet - new_player_bets[current_player_id]\n",
    "            available_stack = new_player_stacks[current_player_id]\n",
    "            actual_call = min(call_amount, available_stack)\n",
    "            new_player_stacks[current_player_id] -= actual_call\n",
    "            new_player_bets[current_player_id] += actual_call\n",
    "            new_pot += actual_call\n",
    "        elif action == Action.RAISE:\n",
    "            raise_amount = amount\n",
    "            total_bet = new_current_bet + raise_amount\n",
    "            bet_amount = total_bet - new_player_bets[current_player_id]\n",
    "            available_stack = new_player_stacks[current_player_id]\n",
    "            actual_bet = min(bet_amount, available_stack)\n",
    "            new_player_stacks[current_player_id] -= actual_bet\n",
    "            new_player_bets[current_player_id] += actual_bet\n",
    "            new_pot += actual_bet\n",
    "            new_current_bet = new_player_bets[current_player_id]\n",
    "            new_last_raiser = current_player_id\n",
    "        elif action == Action.CHECK:\n",
    "            pass  # No action needed\n",
    "        elif action == Action.BET:\n",
    "            bet_amount = amount\n",
    "            available_stack = new_player_stacks[current_player_id]\n",
    "            actual_bet = min(bet_amount, available_stack)\n",
    "            new_player_stacks[current_player_id] -= actual_bet\n",
    "            new_player_bets[current_player_id] += actual_bet\n",
    "            new_pot += actual_bet\n",
    "            new_current_bet = new_player_bets[current_player_id]\n",
    "            new_last_raiser = current_player_id\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        # Determine the next player\n",
    "        next_player = (self.current_player + 1) % self.num_players\n",
    "        while next_player in new_folded_players:\n",
    "            next_player = (next_player + 1) % self.num_players\n",
    "            if next_player == self.current_player:\n",
    "                break  # All other players have folded\n",
    "\n",
    "        # Create a new GameNode with updated state\n",
    "        new_node = GameNode(\n",
    "            current_player=next_player,\n",
    "            hands=new_hands,\n",
    "            board=new_board,\n",
    "            history=new_history,\n",
    "            num_players=self.num_players,\n",
    "            pot=new_pot,\n",
    "            player_stacks=new_player_stacks,\n",
    "            player_bets=new_player_bets,\n",
    "            current_bet=new_current_bet,\n",
    "            folded_players=new_folded_players,\n",
    "            betting_round=new_betting_round,\n",
    "            last_raiser=new_last_raiser,\n",
    "            deck=new_deck\n",
    "        )\n",
    "\n",
    "        # Update betting round if needed\n",
    "        if self.should_progress_round(new_node):\n",
    "            new_node.progress_round()\n",
    "\n",
    "        return new_node\n",
    "\n",
    "    def should_progress_round(self, node):\n",
    "        active_players = [p for p in range(self.num_players) if p not in node.folded_players]\n",
    "        if len(active_players) <= 1:\n",
    "            return True  # Only one player remains\n",
    "        # Check if all active players have matched the current bet or are all-in\n",
    "        for player_id in active_players:\n",
    "            if node.player_stacks[player_id] > 0 and node.player_bets[player_id] != node.current_bet:\n",
    "                return False\n",
    "        if node.last_raiser == node.current_player:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def progress_round(self):\n",
    "        # Reset player bets for the new round\n",
    "        self.player_bets = {player_id: 0 for player_id in self.player_bets}\n",
    "        self.current_bet = 0\n",
    "        self.last_raiser = None\n",
    "\n",
    "        if self.betting_round == 'pre-flop':\n",
    "            self.deal_flop()\n",
    "            self.betting_round = 'flop'\n",
    "        elif self.betting_round == 'flop':\n",
    "            self.deal_turn()\n",
    "            self.betting_round = 'turn'\n",
    "        elif self.betting_round == 'turn':\n",
    "            self.deal_river()\n",
    "            self.betting_round = 'river'\n",
    "        elif self.betting_round == 'river':\n",
    "            self.betting_round = 'showdown'\n",
    "        else:\n",
    "            pass  # Game is over\n",
    "\n",
    "    def deal_flop(self):\n",
    "        # Burn a card\n",
    "        self.deck.pop()\n",
    "        # Deal three community cards\n",
    "        self.board.extend([self.deck.pop() for _ in range(3)])\n",
    "\n",
    "    def deal_turn(self):\n",
    "        # Burn a card\n",
    "        self.deck.pop()\n",
    "        # Deal one community card\n",
    "        self.board.append(self.deck.pop())\n",
    "\n",
    "    def deal_river(self):\n",
    "        # Burn a card\n",
    "        self.deck.pop()\n",
    "        # Deal one community card\n",
    "        self.board.append(self.deck.pop())\n",
    "\n",
    "\n",
    "\n",
    "hands = {\n",
    "    0: [(14, 'h'), (13, 'd')],  # Player 0's hand\n",
    "    1: [(12, 's'), (11, 'c')]   # Player 1's hand\n",
    "}\n",
    "\n",
    "# Create the root game node\n",
    "root_node = GameNode(\n",
    "    current_player=0,\n",
    "    hands=hands,\n",
    "    board=[],\n",
    "    history=[],\n",
    "    num_players=2\n",
    ")\n",
    "\n",
    "# Player 0 takes an action\n",
    "next_node = root_node.take_action(Action.CALL)\n",
    "\n",
    "# Player 1 takes an action\n",
    "next_node = next_node.take_action(Action.CHECK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_net = ValueNetwork(input_size=MAX_FEATURE_LENGTH)\n",
    "# policy_net = PolicyNetwork(input_size=MAX_FEATURE_LENGTH, action_space=len(Action))\n",
    "# value_optimizer = optim.Adam(value_net.parameters(), lr=1e-4)\n",
    "# policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "\n",
    "# NUM_EPISODES = 500\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from enum import Enum, auto\n",
    "\n",
    "# # Define sample_action function\n",
    "# def sample_action(action_probs):\n",
    "#     \"\"\"\n",
    "#     Samples an action from the given action probabilities.\n",
    "\n",
    "#     Args:\n",
    "#         action_probs (torch.Tensor): A tensor containing the probabilities for each action.\n",
    "\n",
    "#     Returns:\n",
    "#         Action: The selected action.\n",
    "#     \"\"\"\n",
    "#     # Ensure action_probs is a 1D tensor\n",
    "#     if action_probs.dim() > 1:\n",
    "#         action_probs = action_probs.squeeze()\n",
    "\n",
    "#     # Sample an action index based on the probabilities\n",
    "#     action_index = torch.multinomial(action_probs, num_samples=1).item()\n",
    "\n",
    "#     # Map the index to an action\n",
    "#     action_list = list(Action)\n",
    "#     selected_action = action_list[action_index]\n",
    "\n",
    "#     return selected_action\n",
    "\n",
    "# # Initialize neural networks and optimizers\n",
    "# value_net = ValueNetwork(input_size=MAX_FEATURE_LENGTH)\n",
    "# policy_net = PolicyNetwork(input_size=MAX_FEATURE_LENGTH, action_space=len(Action))\n",
    "# value_optimizer = optim.Adam(value_net.parameters(), lr=1e-4)\n",
    "# policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "\n",
    "# # Training loop\n",
    "# for episode in range(NUM_EPISODES):\n",
    "#     # Initialize players\n",
    "#     players = [Player(id=0, stack=1000), Player(id=1, stack=1000)]\n",
    "    \n",
    "#     # Initialize the game\n",
    "#     game = TexasHoldEm(players)\n",
    "#     game.deal_hole_cards()\n",
    "    \n",
    "#     # Initialize belief states for each player\n",
    "#     belief_states = {player.id: BeliefState(observed_actions=[], public_cards=game.board) for player in players}\n",
    "    \n",
    "#     done = False\n",
    "#     experiences = []  # To store experiences for training\n",
    "    \n",
    "#     while not done:\n",
    "#         current_player = game.get_current_player()\n",
    "#         belief_state = belief_states[current_player.id]\n",
    "        \n",
    "#         # Extract features\n",
    "#         features = extract_features(belief_state)\n",
    "        \n",
    "#         # Get action probabilities from the policy network\n",
    "#         action_probs = policy_net(features)\n",
    "        \n",
    "#         # Choose an action\n",
    "#         action = sample_action(action_probs)\n",
    "        \n",
    "#         # Execute the action\n",
    "#         game.execute_action(current_player, action)\n",
    "        \n",
    "#         # Get reward (to be defined based on your reward structure)\n",
    "#         reward = game.get_reward(current_player)\n",
    "        \n",
    "#         # Record experience\n",
    "#         experiences.append((features, action, reward))\n",
    "        \n",
    "#         # Update belief states for all players\n",
    "#         for player in players:\n",
    "#             belief_states[player.id].update(action, new_public_cards=game.board)\n",
    "        \n",
    "#         # Check for end of round/game\n",
    "#         if game.is_round_over():\n",
    "#             game.progress_round()\n",
    "#         if game.is_game_over():\n",
    "#             game.determine_winner()\n",
    "#             done = True  # End the game loop\n",
    "    \n",
    "#     # After the game, update the networks using collected experiences\n",
    "\n",
    "#     # Step 1: Prepare the training data\n",
    "#     states = torch.stack([exp[0] for exp in experiences])  # Features are tensors\n",
    "#     actions = [exp[1] for exp in experiences]              # Actions are enums\n",
    "#     rewards = [exp[2] for exp in experiences]              # Rewards are scalars\n",
    "\n",
    "#     # Step 2: Convert actions to indices\n",
    "#     action_to_index = {action: idx for idx, action in enumerate(Action)}\n",
    "#     action_indices = torch.tensor([action_to_index[action] for action in actions], dtype=torch.long)\n",
    "\n",
    "#     # Step 3: Compute value targets (discounted cumulative rewards)\n",
    "#     rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "#     gamma = 1.0  # No discounting\n",
    "#     returns = []\n",
    "#     R = 0\n",
    "#     for r in reversed(rewards):\n",
    "#         R = r + gamma * R\n",
    "#         returns.insert(0, R)\n",
    "#     value_targets = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "#     # Step 4: Compute value loss\n",
    "#     value_predictions = value_net(states).squeeze()\n",
    "#     value_loss_fn = nn.MSELoss()\n",
    "#     value_loss = value_loss_fn(value_predictions, value_targets)\n",
    "\n",
    "#     # Step 5: Compute policy loss\n",
    "#     policy_outputs = policy_net(states)  # Shape: (batch_size, num_actions)\n",
    "#     action_probs = policy_outputs.gather(1, action_indices.unsqueeze(1)).squeeze()\n",
    "#     log_probs = torch.log(action_probs + 1e-10)\n",
    "#     with torch.no_grad():\n",
    "#         advantages = value_targets - value_predictions.detach()\n",
    "#     policy_loss = - (log_probs * advantages).mean()\n",
    "\n",
    "#     # Step 6: Update the networks\n",
    "#     # Update value network\n",
    "#     value_optimizer.zero_grad()\n",
    "#     value_loss.backward()\n",
    "#     value_optimizer.step()\n",
    "\n",
    "#     # Update policy network\n",
    "#     policy_optimizer.zero_grad()\n",
    "#     policy_loss.backward()\n",
    "#     policy_optimizer.step()\n",
    "    \n",
    "#     # Compute entropy of the policy\n",
    "#     entropy = - (policy_outputs * torch.log(policy_outputs + 1e-10)).sum(dim=1).mean()\n",
    "\n",
    "#     # Add entropy regularization to the policy loss\n",
    "#     entropy_coef = 0.01  # Adjust as needed\n",
    "#     policy_loss = policy_loss - entropy_coef * entropy\n",
    "    \n",
    "#     # Extract CFR strategies and values\n",
    "#     cfr_strategies = torch.stack([exp[2] for exp in experiences])  # CFR strategies\n",
    "#     cfr_values = torch.tensor([exp[3] for exp in experiences], dtype=torch.float32)  # CFR values\n",
    "\n",
    "#     # Value Loss\n",
    "#     value_predictions = value_net(states).squeeze()\n",
    "#     value_loss_fn = nn.MSELoss()\n",
    "#     value_loss = value_loss_fn(value_predictions, cfr_values)\n",
    "\n",
    "#     # Policy Loss\n",
    "#     policy_outputs = policy_net(states)\n",
    "#     policy_loss_fn = nn.CrossEntropyLoss()\n",
    "#     policy_loss = policy_loss_fn(policy_outputs, cfr_strategies.argmax(dim=1))\n",
    "\n",
    "\n",
    "\n",
    "#     # Optionally, print progress\n",
    "#     if (episode + 1) % 100 == 0:\n",
    "#         print(f\"Completed episode {episode + 1}/{NUM_EPISODES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "MAX_FEATURE_LENGTH = 25\n",
    "\n",
    "# Define ValueNetwork\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Output a single value\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define PolicyNetwork\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, action_space):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_space)\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "# Initialize neural networks and optimizers\n",
    "value_net = ValueNetwork(input_size=MAX_FEATURE_LENGTH)\n",
    "policy_net = PolicyNetwork(input_size=MAX_FEATURE_LENGTH, action_space=len(Action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, id, stack):\n",
    "        self.id = id\n",
    "        self.stack = stack  # Total chips the player has\n",
    "        self.hand = []      # The player's hole cards\n",
    "        self.current_bet = 0\n",
    "        self.folded = False\n",
    "\n",
    "    def reset_for_new_hand(self):\n",
    "        self.hand = []\n",
    "        self.current_bet = 0\n",
    "        self.folded = False\n",
    "\n",
    "import random\n",
    "from enum import Enum\n",
    "\n",
    "class TexasHoldEm:\n",
    "    def __init__(self, players, small_blind=10, big_blind=20):\n",
    "        self.players = players\n",
    "        self.small_blind = small_blind\n",
    "        self.big_blind = big_blind\n",
    "        self.deck = self.initialize_deck()\n",
    "        self.board = []  # Community cards\n",
    "        self.pot = 0\n",
    "        self.current_bet = 0\n",
    "        self.game_over = False\n",
    "        self.dealer_index = 0\n",
    "        self.current_player_index = (self.dealer_index + 1) % len(self.players)\n",
    "        self.betting_round = 'pre-flop'\n",
    "        self.round_bets = {player.id: 0 for player in self.players}\n",
    "        self.last_raiser = None\n",
    "\n",
    "    def initialize_deck(self):\n",
    "        suits = ['h', 'd', 'c', 's']\n",
    "        ranks = list(range(2, 15))  # 2-14 where 11-14 are J, Q, K, A\n",
    "        deck = [(rank, suit) for rank in ranks for suit in suits]\n",
    "        random.shuffle(deck)\n",
    "        return deck\n",
    "\n",
    "    def deal_hole_cards(self):\n",
    "        for player in self.players:\n",
    "            player.hand = [self.deck.pop(), self.deck.pop()]\n",
    "            player.current_bet = 0\n",
    "            player.folded = False\n",
    "        self.round_bets = {player.id: 0 for player in self.players}\n",
    "\n",
    "    def post_blinds(self):\n",
    "        small_blind_player = self.players[(self.dealer_index + 1) % len(self.players)]\n",
    "        big_blind_player = self.players[(self.dealer_index + 2) % len(self.players)]\n",
    "        self._post_blind(small_blind_player, self.small_blind)\n",
    "        self._post_blind(big_blind_player, self.big_blind)\n",
    "        self.current_bet = self.big_blind\n",
    "        self.last_raiser = big_blind_player.id\n",
    "        self.current_player_index = (self.dealer_index + 3) % len(self.players)\n",
    "\n",
    "    def _post_blind(self, player, amount):\n",
    "        player.stack -= amount\n",
    "        player.current_bet = amount\n",
    "        self.pot += amount\n",
    "        self.round_bets[player.id] += amount\n",
    "\n",
    "    def get_current_player(self):\n",
    "        while True:\n",
    "            player = self.players[self.current_player_index]\n",
    "            if not player.folded and player.stack > 0:\n",
    "                return player\n",
    "            self.current_player_index = (self.current_player_index + 1) % len(self.players)\n",
    "\n",
    "    def get_available_actions(self, player):\n",
    "        actions = []\n",
    "        if player.current_bet < self.current_bet:\n",
    "            actions.extend([Action.FOLD, Action.CALL])\n",
    "            if player.stack > (self.current_bet - player.current_bet):\n",
    "                actions.append(Action.RAISE)\n",
    "        else:\n",
    "            actions.append(Action.CHECK)\n",
    "            if player.stack > 0:\n",
    "                actions.append(Action.BET)\n",
    "        return actions\n",
    "\n",
    "    def execute_action(self, player, action, raise_amount=0):\n",
    "        if action == Action.FOLD:\n",
    "            self.handle_fold(player)\n",
    "        elif action == Action.CHECK:\n",
    "            self.handle_check(player)\n",
    "        elif action == Action.CALL:\n",
    "            self.handle_call(player)\n",
    "        elif action == Action.BET:\n",
    "            self.handle_bet(player, raise_amount)\n",
    "        elif action == Action.RAISE:\n",
    "            self.handle_raise(player, raise_amount)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        # Move to the next player\n",
    "        self.current_player_index = (self.current_player_index + 1) % len(self.players)\n",
    "\n",
    "    def handle_fold(self, player):\n",
    "        player.folded = True\n",
    "\n",
    "    def handle_check(self, player):\n",
    "        pass  # No action needed\n",
    "\n",
    "    def handle_call(self, player):\n",
    "        call_amount = self.current_bet - player.current_bet\n",
    "        actual_call = min(call_amount, player.stack)\n",
    "        player.stack -= actual_call\n",
    "        player.current_bet += actual_call\n",
    "        self.pot += actual_call\n",
    "        self.round_bets[player.id] += actual_call\n",
    "\n",
    "    def handle_bet(self, player, amount):\n",
    "        if amount <= 0 or amount > player.stack:\n",
    "            raise ValueError(\"Invalid bet amount\")\n",
    "        player.stack -= amount\n",
    "        player.current_bet += amount\n",
    "        self.current_bet = player.current_bet\n",
    "        self.pot += amount\n",
    "        self.round_bets[player.id] += amount\n",
    "        self.last_raiser = player.id\n",
    "\n",
    "    def handle_raise(self, player, amount):\n",
    "        if amount <= 0 or amount > player.stack:\n",
    "            raise ValueError(\"Invalid raise amount\")\n",
    "        raise_amount = amount\n",
    "        call_amount = self.current_bet - player.current_bet\n",
    "        total_bet = call_amount + raise_amount\n",
    "        if total_bet > player.stack:\n",
    "            total_bet = player.stack\n",
    "        player.stack -= total_bet\n",
    "        player.current_bet += total_bet\n",
    "        self.current_bet = player.current_bet\n",
    "        self.pot += total_bet\n",
    "        self.round_bets[player.id] += total_bet\n",
    "        self.last_raiser = player.id\n",
    "\n",
    "    def is_round_over(self):\n",
    "        active_players = [p for p in self.players if not p.folded and p.stack > 0]\n",
    "        if len(active_players) <= 1:\n",
    "            return True\n",
    "        for player in active_players:\n",
    "            if player.current_bet != self.current_bet:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def progress_round(self):\n",
    "        for player in self.players:\n",
    "            player.current_bet = 0\n",
    "        self.round_bets = {player.id: 0 for player in self.players}\n",
    "        self.current_bet = 0\n",
    "        self.last_raiser = None\n",
    "\n",
    "        if self.betting_round == 'pre-flop':\n",
    "            self.deal_flop()\n",
    "            self.betting_round = 'flop'\n",
    "        elif self.betting_round == 'flop':\n",
    "            self.deal_turn()\n",
    "            self.betting_round = 'turn'\n",
    "        elif self.betting_round == 'turn':\n",
    "            self.deal_river()\n",
    "            self.betting_round = 'river'\n",
    "        elif self.betting_round == 'river':\n",
    "            self.game_over = True\n",
    "\n",
    "    def deal_flop(self):\n",
    "        self.deck.pop()  # Burn card\n",
    "        self.board.extend([self.deck.pop() for _ in range(3)])\n",
    "\n",
    "    def deal_turn(self):\n",
    "        self.deck.pop()  # Burn card\n",
    "        self.board.append(self.deck.pop())\n",
    "\n",
    "    def deal_river(self):\n",
    "        self.deck.pop()  # Burn card\n",
    "        self.board.append(self.deck.pop())\n",
    "\n",
    "    def is_game_over(self):\n",
    "        active_players = [p for p in self.players if not p.folded and p.stack > 0]\n",
    "        if len(active_players) <= 1:\n",
    "            return True\n",
    "        return self.game_over\n",
    "\n",
    "    def determine_winner(self):\n",
    "        active_players = [p for p in self.players if not p.folded]\n",
    "        if len(active_players) == 1:\n",
    "            winner = active_players[0]\n",
    "            winner.stack += self.pot\n",
    "            self.pot = 0\n",
    "        else:\n",
    "            # Showdown: evaluate hands\n",
    "            from treys import Evaluator, Card\n",
    "            evaluator = Evaluator()\n",
    "            best_rank = None\n",
    "            winners = []\n",
    "            for player in active_players:\n",
    "                player_cards = [Card.new(f\"{self.rank_to_str(rank)}{suit}\") for rank, suit in player.hand]\n",
    "                board_cards = [Card.new(f\"{self.rank_to_str(rank)}{suit}\") for rank, suit in self.board]\n",
    "                hand_rank = evaluator.evaluate(board_cards, player_cards)\n",
    "                if best_rank is None or hand_rank < best_rank:\n",
    "                    best_rank = hand_rank\n",
    "                    winners = [player]\n",
    "                elif hand_rank == best_rank:\n",
    "                    winners.append(player)\n",
    "            split_pot = self.pot / len(winners)\n",
    "            for winner in winners:\n",
    "                winner.stack += split_pot\n",
    "            self.pot = 0\n",
    "\n",
    "    def rank_to_str(self, rank):\n",
    "        rank_dict = {14: 'A', 13: 'K', 12: 'Q', 11: 'J', 10: 'T'}\n",
    "        return rank_dict.get(rank, str(rank))\n",
    "\n",
    "    def get_reward(self, player):\n",
    "        initial_stack = 1000\n",
    "        return player.stack - initial_stack\n",
    "\n",
    "class BeliefState:\n",
    "    def __init__(self, observed_actions, public_cards, pot_size=0):\n",
    "        self.observed_actions = observed_actions\n",
    "        self.public_cards = public_cards\n",
    "        self.pot_size = pot_size  # Add pot size\n",
    "\n",
    "    def update(self, action, new_public_cards, pot_size):\n",
    "        self.observed_actions.append(action)\n",
    "        self.public_cards = new_public_cards\n",
    "        self.pot_size = pot_size\n",
    "\n",
    "def extract_features(belief_state):\n",
    "    features = []\n",
    "\n",
    "    # Encode observed actions\n",
    "    action_encoding = {\n",
    "        Action.FOLD: 0,\n",
    "        Action.CHECK: 1,\n",
    "        Action.CALL: 2,\n",
    "        Action.RAISE: 3,\n",
    "        Action.BET: 4\n",
    "    }\n",
    "    max_history_length = 12  # Increased from 10 to 12\n",
    "    action_features = [action_encoding[action] for action in belief_state.observed_actions[-max_history_length:]]\n",
    "    action_features += [0] * (max_history_length - len(action_features))\n",
    "    features.extend(action_features)\n",
    "\n",
    "    # Encode public cards\n",
    "    rank_encoding = {r: i for i, r in enumerate(range(2, 15), start=1)}\n",
    "    suit_encoding = {'h': 0, 'd': 1, 'c': 2, 's': 3}\n",
    "    max_board_cards = 6  # Increased from 5 to 6\n",
    "    board_features = []\n",
    "    for rank, suit in belief_state.public_cards:\n",
    "        rank_feature = rank_encoding.get(rank, 0)\n",
    "        suit_feature = suit_encoding.get(suit, 0)\n",
    "        board_features.extend([rank_feature, suit_feature])\n",
    "    while len(board_features) < max_board_cards * 2:\n",
    "        board_features.extend([0, 0])\n",
    "    features.extend(board_features)\n",
    "\n",
    "    # Ensure total feature length is 25\n",
    "    # Current length: max_history_length (12) + max_board_cards * 2 (12) = 24\n",
    "    # Add one more feature, e.g., pot size\n",
    "    pot_size = belief_state.pot_size if hasattr(belief_state, 'pot_size') else 0\n",
    "    features.append(pot_size / 1000)  # Normalize pot size\n",
    "\n",
    "    # Convert to tensor\n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jasje\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed episode 100/500\n",
      "Completed episode 200/500\n",
      "Completed episode 300/500\n",
      "Completed episode 400/500\n",
      "Completed episode 500/500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from enum import Enum, auto\n",
    "\n",
    "class Action(Enum):\n",
    "    FOLD = 0\n",
    "    CHECK = 1\n",
    "    CALL = 2\n",
    "    RAISE = 3\n",
    "    BET = 4\n",
    "\n",
    "# Define MAX_FEATURE_LENGTH\n",
    "MAX_FEATURE_LENGTH = 25  # Adjust as needed\n",
    "\n",
    "# Define sample_action function\n",
    "def sample_action(action_probs, valid_actions):\n",
    "    \"\"\"\n",
    "    Samples an action from the given action probabilities, considering only valid actions.\n",
    "\n",
    "    Args:\n",
    "        action_probs (torch.Tensor): A tensor containing the probabilities for each action.\n",
    "        valid_actions (list of Action): List of valid actions in the current state.\n",
    "\n",
    "    Returns:\n",
    "        Action: The selected action.\n",
    "    \"\"\"\n",
    "    action_list = list(Action)\n",
    "    action_to_index = {action: idx for idx, action in enumerate(action_list)}\n",
    "    valid_action_indices = [action_to_index[action] for action in valid_actions]\n",
    "\n",
    "    # Get probabilities of valid actions\n",
    "    valid_action_probs = action_probs[valid_action_indices]\n",
    "    valid_action_probs /= valid_action_probs.sum()  # Normalize\n",
    "\n",
    "    # Sample from valid actions\n",
    "    chosen_index = torch.multinomial(valid_action_probs, num_samples=1).item()\n",
    "    selected_action = valid_actions[chosen_index]\n",
    "\n",
    "    return selected_action\n",
    "\n",
    "def determine_raise_amount(player, game):\n",
    "    \"\"\"\n",
    "    Determines the amount to raise or bet.\n",
    "\n",
    "    Args:\n",
    "        player (Player): The player who is raising.\n",
    "        game (TexasHoldEm): The current game state.\n",
    "\n",
    "    Returns:\n",
    "        float: The raise amount.\n",
    "    \"\"\"\n",
    "    # Simple strategy: raise by a fixed amount or percentage of the pot\n",
    "    # For this example, we'll raise by half the pot or the player's remaining stack, whichever is smaller\n",
    "    raise_amount = min(player.stack, max(game.pot * 0.5, game.big_blind))\n",
    "    return raise_amount\n",
    "\n",
    "# Initialize neural networks and optimizers\n",
    "value_net = ValueNetwork(input_size=MAX_FEATURE_LENGTH)\n",
    "policy_net = PolicyNetwork(input_size=MAX_FEATURE_LENGTH, action_space=len(Action))\n",
    "value_optimizer = optim.Adam(value_net.parameters(), lr=1e-4)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)\n",
    "\n",
    "NUM_EPISODES = 500\n",
    "\n",
    "# Training loop\n",
    "for episode in range(NUM_EPISODES):\n",
    "    # Initialize players\n",
    "    players = [Player(id=0, stack=1000), Player(id=1, stack=1000)]\n",
    "    \n",
    "    # Initialize the game\n",
    "    game = TexasHoldEm(players)\n",
    "    game.deal_hole_cards()\n",
    "    game.post_blinds()\n",
    "    \n",
    "    # Initialize belief states for each player\n",
    "    belief_states = {\n",
    "        player.id: BeliefState(\n",
    "            observed_actions=[],\n",
    "            public_cards=game.board,\n",
    "            pot_size=game.pot  # Optional\n",
    "        ) for player in players\n",
    "    }\n",
    "    \n",
    "    done = False\n",
    "    experiences = {player.id: [] for player in players}  # Collect experiences for each player separately\n",
    "    \n",
    "    while not done:\n",
    "        current_player = game.get_current_player()\n",
    "        belief_state = belief_states[current_player.id]\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_features(belief_state)\n",
    "\n",
    "        # Ensure features have the correct shape\n",
    "        if features.dim() == 1:\n",
    "            features = features.unsqueeze(0)  # Add batch dimension if necessary\n",
    "        \n",
    "        # Get action probabilities from the policy network\n",
    "        action_probs = policy_net(features)\n",
    "        action_probs = action_probs.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Get available actions for the current player\n",
    "        valid_actions = game.get_available_actions(current_player)\n",
    "        \n",
    "        # Choose an action\n",
    "        action = sample_action(action_probs, valid_actions)\n",
    "        \n",
    "        # Determine raise amount if necessary\n",
    "        if action in [Action.BET, Action.RAISE]:\n",
    "            raise_amount = determine_raise_amount(current_player, game)\n",
    "            game.execute_action(current_player, action, raise_amount=raise_amount)\n",
    "        else:\n",
    "            game.execute_action(current_player, action)\n",
    "        \n",
    "        # Record experience for the current player\n",
    "        experiences[current_player.id].append((features.squeeze(0), action, 0))  # Squeeze features before storing, and reward is zero for now\n",
    "        \n",
    "        # Update belief states for all players\n",
    "        for player in players:\n",
    "            belief_states[player.id].update(\n",
    "                action,\n",
    "                new_public_cards=game.board,\n",
    "                pot_size=game.pot  # Add this argument\n",
    "            )\n",
    "        \n",
    "        # Check for end of round/game\n",
    "        if game.is_round_over():\n",
    "            game.progress_round()\n",
    "        if game.is_game_over():\n",
    "            game.determine_winner()\n",
    "            done = True  # End the game loop\n",
    "    \n",
    "    # After the game ends, assign rewards and update networks\n",
    "    \n",
    "    # Assign final rewards to each player's experiences\n",
    "    # Update belief states for all players\n",
    "    for player in players:\n",
    "        belief_states[player.id].update(\n",
    "            action,\n",
    "            new_public_cards=game.board,\n",
    "            pot_size=game.pot\n",
    "        )\n",
    "    \n",
    "    # Combine experiences from both players\n",
    "    all_experiences = []\n",
    "    for exp_list in experiences.values():\n",
    "        all_experiences.extend(exp_list)\n",
    "    \n",
    "    # Proceed to prepare data and update networks\n",
    "\n",
    "    # Step 1: Prepare the training data\n",
    "    states = torch.stack([exp[0] for exp in all_experiences])  # Features are tensors\n",
    "    actions = [exp[1] for exp in all_experiences]              # Actions are enums\n",
    "    rewards = [exp[2] for exp in all_experiences]              # Rewards are scalars\n",
    "\n",
    "    # Step 2: Convert actions to indices\n",
    "    action_to_index = {action: idx for idx, action in enumerate(Action)}\n",
    "    action_indices = torch.tensor([action_to_index[action] for action in actions], dtype=torch.long)\n",
    "\n",
    "    # Step 3: Compute value targets\n",
    "    value_targets = torch.tensor(rewards, dtype=torch.float32)\n",
    "    \n",
    "    # Step 4: Compute value loss\n",
    "    value_predictions = value_net(states).squeeze()\n",
    "    value_loss_fn = nn.MSELoss()\n",
    "    value_loss = value_loss_fn(value_predictions, value_targets)\n",
    "\n",
    "    # Step 5: Compute policy loss\n",
    "    policy_outputs = policy_net(states)  # Shape: (batch_size, num_actions)\n",
    "    \n",
    "    action_probs_taken = policy_outputs.gather(1, action_indices.unsqueeze(1)).squeeze()\n",
    "    log_probs = torch.log(action_probs_taken + 1e-10)\n",
    "    with torch.no_grad():\n",
    "        advantages = value_targets - value_predictions.detach()\n",
    "    policy_loss = - (log_probs * advantages).mean()\n",
    "    \n",
    "    # Optional: Add entropy regularization to encourage exploration\n",
    "    entropy = - (policy_outputs * torch.log(policy_outputs + 1e-10)).sum(dim=1).mean()\n",
    "    entropy_coef = 0.01  # Adjust as needed\n",
    "    policy_loss = policy_loss - entropy_coef * entropy\n",
    "\n",
    "    # Step 6: Update the networks\n",
    "    # Update value network\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    # Update policy network\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    # Optionally, print progress\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Completed episode {episode + 1}/{NUM_EPISODES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is complete\n",
    "torch.save(policy_net.state_dict(), 'policy_net.pth')\n",
    "torch.save(value_net.state_dict(), 'value_net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 7) (3913444657.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 7\u001b[1;36m\u001b[0m\n\u001b[1;33m    To implement your AI and play against it, you'll need to:\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 7)\n"
     ]
    }
   ],
   "source": [
    "Congratulations on getting your training loop running! Now, let's focus on how to use your trained AI model to play a game of Texas Hold'em, either against another AI or a human player.\n",
    "\n",
    "---\n",
    "\n",
    "### *Overview*\n",
    "\n",
    "To implement your AI and play against it, you'll need to:\n",
    "\n",
    "1. *Save the Trained Models*: Save your policy_net and value_net after training.\n",
    "\n",
    "2. *Load the Models*: In a new script or the same script, load the saved models.\n",
    "\n",
    "3. *Set Up the Game Loop*: Create a game loop where the AI can interact with other players.\n",
    "\n",
    "4. *Provide Necessary Inputs*: Ensure that the AI receives the required inputs at each turn.\n",
    "\n",
    "5. *Integrate the AI's Decisions*: Use the policy_net to make decisions based on the current game state.\n",
    "\n",
    "6. *Handle User Input*: If playing against a human, capture the user's actions.\n",
    "\n",
    "7. *Maintain Game State*: Keep track of blinds, bets, community cards, and player stacks.\n",
    "\n",
    "---\n",
    "\n",
    "### *Step 1: Save the Trained Models*\n",
    "\n",
    "After your training loop completes, save your models using PyTorch's torch.save function.\n",
    "\n",
    "python\n",
    "# After training is complete\n",
    "torch.save(policy_net.state_dict(), 'policy_net.pth')\n",
    "torch.save(value_net.state_dict(), 'value_net.pth')\n",
    "\n",
    "\n",
    "You can add this code at the end of your training script.\n",
    "\n",
    "---\n",
    "\n",
    "### *Step 2: Load the Models*\n",
    "\n",
    "In your new script (or a new cell if you're using a notebook), load the saved models.\n",
    "\n",
    "python\n",
    "import torch\n",
    "\n",
    "# Define MAX_FEATURE_LENGTH and Action enum as before\n",
    "MAX_FEATURE_LENGTH = 25\n",
    "\n",
    "class Action(Enum):\n",
    "    FOLD = 0\n",
    "    CHECK = 1\n",
    "    CALL = 2\n",
    "    RAISE = 3\n",
    "    BET = 4\n",
    "\n",
    "# Initialize neural networks\n",
    "policy_net = PolicyNetwork(input_size=MAX_FEATURE_LENGTH, action_space=len(Action))\n",
    "value_net = ValueNetwork(input_size=MAX_FEATURE_LENGTH)\n",
    "\n",
    "# Load the saved models\n",
    "policy_net.load_state_dict(torch.load('policy_net.pth'))\n",
    "value_net.load_state_dict(torch.load('value_net.pth'))\n",
    "\n",
    "# Set networks to evaluation mode\n",
    "policy_net.eval()\n",
    "value_net.eval()\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Step 3: Set Up the Game Loop*\n",
    "\n",
    "Create a game loop where the AI can play against another player (AI or human). We'll assume a two-player game: the AI and a human player.\n",
    "\n",
    "python\n",
    "# Initialize players\n",
    "ai_player = Player(id=0, stack=1000)\n",
    "human_player = Player(id=1, stack=1000)\n",
    "players = [ai_player, human_player]\n",
    "\n",
    "# Initialize the game\n",
    "game = TexasHoldEm(players)\n",
    "game.deal_hole_cards()\n",
    "game.post_blinds()\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Step 4: Provide Necessary Inputs to the AI*\n",
    "\n",
    "At each turn, you need to construct the BeliefState for the AI and extract features for the policy_net.\n",
    "\n",
    "#### *Constructing the BeliefState*\n",
    "\n",
    "python\n",
    "ai_belief_state = BeliefState(\n",
    "    observed_actions=[],   # Actions observed so far\n",
    "    public_cards=game.board,\n",
    "    pot_size=game.pot\n",
    ")\n",
    "\n",
    "\n",
    "Update the ai_belief_state as the game progresses by appending actions and updating the pot size.\n",
    "\n",
    "---\n",
    "\n",
    "### *Step 5: Integrate the AI's Decisions*\n",
    "\n",
    "Create a function for the AI to decide on an action based on the current game state.\n",
    "\n",
    "python\n",
    "def ai_decide_action(ai_player, game, ai_belief_state):\n",
    "    # Extract features\n",
    "    features = extract_features(ai_belief_state)\n",
    "    if features.dim() == 1:\n",
    "        features = features.unsqueeze(0)\n",
    "    \n",
    "    # Get action probabilities\n",
    "    with torch.no_grad():\n",
    "        action_probs = policy_net(features)\n",
    "    action_probs = action_probs.squeeze(0)\n",
    "    \n",
    "    # Get available actions\n",
    "    valid_actions = game.get_available_actions(ai_player)\n",
    "    \n",
    "    # Choose an action\n",
    "    action = sample_action(action_probs, valid_actions)\n",
    "    \n",
    "    # Determine raise amount if necessary\n",
    "    if action in [Action.BET, Action.RAISE]:\n",
    "        raise_amount = determine_raise_amount(ai_player, game)\n",
    "        game.execute_action(ai_player, action, raise_amount=raise_amount)\n",
    "    else:\n",
    "        game.execute_action(ai_player, action)\n",
    "    \n",
    "    return action\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Step 6: Handle User Input*\n",
    "\n",
    "For the human player, prompt them to enter their action.\n",
    "\n",
    "python\n",
    "def human_decide_action(human_player, game):\n",
    "    valid_actions = game.get_available_actions(human_player)\n",
    "    print(f\"Your hand: {human_player.hand}\")\n",
    "    print(f\"Community cards: {game.board}\")\n",
    "    print(f\"Pot size: {game.pot}\")\n",
    "    print(f\"Your stack: {human_player.stack}\")\n",
    "    print(f\"Current bet: {game.current_bet}\")\n",
    "    print(f\"Valid actions: {[action.name for action in valid_actions]}\")\n",
    "    \n",
    "    action = None\n",
    "    while action not in valid_actions:\n",
    "        action_input = input(\"Enter your action: \").upper()\n",
    "        try:\n",
    "            action = Action[action_input]\n",
    "            if action not in valid_actions:\n",
    "                print(\"Invalid action. Please choose from the valid actions.\")\n",
    "        except KeyError:\n",
    "            print(\"Invalid action name. Please try again.\")\n",
    "    \n",
    "    # Determine raise amount if necessary\n",
    "    if action in [Action.BET, Action.RAISE]:\n",
    "        raise_amount = float(input(\"Enter raise amount: \"))\n",
    "        game.execute_action(human_player, action, raise_amount=raise_amount)\n",
    "    else:\n",
    "        game.execute_action(human_player, action)\n",
    "    \n",
    "    return action\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Step 7: Update Belief States and Game State*\n",
    "\n",
    "After each action, update the belief states for both players.\n",
    "\n",
    "python\n",
    "def update_belief_states(action, game, ai_belief_state, human_belief_state):\n",
    "    # Update AI's belief state\n",
    "    ai_belief_state.update(\n",
    "        action,\n",
    "        new_public_cards=game.board,\n",
    "        pot_size=game.pot\n",
    "    )\n",
    "    \n",
    "    # Update human player's belief state if needed\n",
    "    # For simplicity, we can skip this if not used\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Step 8: Complete the Game Loop*\n",
    "\n",
    "Now, integrate everything into a game loop.\n",
    "\n",
    "python\n",
    "# Initialize belief states\n",
    "ai_belief_state = BeliefState(\n",
    "    observed_actions=[],\n",
    "    public_cards=game.board,\n",
    "    pot_size=game.pot\n",
    ")\n",
    "\n",
    "# For the human player, you can have a similar belief state if needed\n",
    "human_belief_state = None  # Not required unless you need it\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    current_player = game.get_current_player()\n",
    "    \n",
    "    if current_player == ai_player:\n",
    "        action = ai_decide_action(ai_player, game, ai_belief_state)\n",
    "        print(f\"AI chooses to {action.name}\")\n",
    "    else:\n",
    "        action = human_decide_action(human_player, game)\n",
    "    \n",
    "    # Update belief states\n",
    "    update_belief_states(action, game, ai_belief_state, human_belief_state)\n",
    "    \n",
    "    # Check for end of round/game\n",
    "    if game.is_round_over():\n",
    "        game.progress_round()\n",
    "    if game.is_game_over():\n",
    "        game.determine_winner()\n",
    "        done = True  # End the game loop\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Step 9: Handling Blinds and Initial Variables*\n",
    "\n",
    "At the start of the game, you need to handle the blinds.\n",
    "\n",
    "python\n",
    "# Decide who is small blind and big blind\n",
    "small_blind_player = players[0]  # Could be randomized\n",
    "big_blind_player = players[1]\n",
    "\n",
    "game.post_blinds()\n",
    "\n",
    "# Update AI belief state with initial blinds if necessary\n",
    "ai_belief_state.observed_actions.append(Action.BET)  # Assuming blinds are considered bets\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Step 10: Implementing Required Inputs*\n",
    "\n",
    "From your requirements, the AI needs to know:\n",
    "\n",
    "- *If it's the small blind or big blind*: You can set a flag in the Player class or pass this information to the AI.\n",
    "\n",
    "- *Current blind to be called*: The AI can get this from game.current_bet.\n",
    "\n",
    "- *What other players have bet*: The game.round_bets or player.current_bet can provide this information.\n",
    "\n",
    "- *Current bid value on the table*: Available via game.current_bet.\n",
    "\n",
    "- *Community cards*: Already included in the BeliefState.\n",
    "\n",
    "- *Your current money/points*: Available via player.stack.\n",
    "\n",
    "- *Your hand*: For the AI, you may need to include private cards in the features if necessary.\n",
    "\n",
    "---\n",
    "\n",
    "### **Enhancing the extract_features Function**\n",
    "\n",
    "To include additional inputs, modify the extract_features function.\n",
    "\n",
    "python\n",
    "def extract_features(belief_state, player):\n",
    "    features = []\n",
    "    \n",
    "    # Encode observed actions (same as before)\n",
    "    # ...\n",
    "    \n",
    "    # Encode public cards (same as before)\n",
    "    # ...\n",
    "    \n",
    "    # Encode player's stack\n",
    "    features.append(player.stack / 1000)  # Normalize if needed\n",
    "    \n",
    "    # Encode current bet\n",
    "    features.append(player.current_bet / 1000)\n",
    "    \n",
    "    # Encode player's hand (if you choose to include private cards)\n",
    "    # For example, encode ranks and suits of hole cards\n",
    "    hole_card_features = []\n",
    "    rank_encoding = {r: i for i, r in enumerate(range(2, 15), start=1)}\n",
    "    suit_encoding = {'h': 0, 'd': 1, 'c': 2, 's': 3}\n",
    "    for rank, suit in player.hand:\n",
    "        rank_feature = rank_encoding.get(rank, 0)\n",
    "        suit_feature = suit_encoding.get(suit, 0)\n",
    "        hole_card_features.extend([rank_feature, suit_feature])\n",
    "    features.extend(hole_card_features)\n",
    "    \n",
    "    # Ensure total feature length matches MAX_FEATURE_LENGTH\n",
    "    # Pad with zeros if necessary\n",
    "    features = features[:MAX_FEATURE_LENGTH] + [0] * max(0, MAX_FEATURE_LENGTH - len(features))\n",
    "    \n",
    "    # Convert to tensor\n",
    "    features = torch.tensor(features, dtype=torch.float32)\n",
    "    return features\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Adjusting MAX_FEATURE_LENGTH*\n",
    "\n",
    "After adding new features, ensure that MAX_FEATURE_LENGTH is updated to match the total number of features.\n",
    "\n",
    "For example, if you've added:\n",
    "\n",
    "- 2 features for the player's stack and current bet\n",
    "- 4 features for the hole cards (2 cards * 2 features each)\n",
    "\n",
    "Update MAX_FEATURE_LENGTH accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### *Saving and Loading Models*\n",
    "\n",
    "As you mentioned, you might want to have a separate Python file to run the game. Ensure you have the necessary code to load the models in that file.\n",
    "\n",
    "---\n",
    "\n",
    "### *Putting It All Together*\n",
    "\n",
    "Here's a simplified version of the game loop:\n",
    "\n",
    "python\n",
    "def play_game():\n",
    "    # Initialize players\n",
    "    ai_player = Player(id=0, stack=1000)\n",
    "    human_player = Player(id=1, stack=1000)\n",
    "    players = [ai_player, human_player]\n",
    "    \n",
    "    # Initialize the game\n",
    "    game = TexasHoldEm(players)\n",
    "    game.deal_hole_cards()\n",
    "    game.post_blinds()\n",
    "    \n",
    "    # Initialize belief states\n",
    "    ai_belief_state = BeliefState(\n",
    "        observed_actions=[],\n",
    "        public_cards=game.board,\n",
    "        pot_size=game.pot\n",
    "    )\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        current_player = game.get_current_player()\n",
    "        \n",
    "        if current_player == ai_player:\n",
    "            action = ai_decide_action(ai_player, game, ai_belief_state)\n",
    "            print(f\"AI chooses to {action.name}\")\n",
    "        else:\n",
    "            action = human_decide_action(human_player, game)\n",
    "        \n",
    "        # Update belief states\n",
    "        update_belief_states(action, game, ai_belief_state, None)\n",
    "        \n",
    "        # Check for end of round/game\n",
    "        if game.is_round_over():\n",
    "            game.progress_round()\n",
    "        if game.is_game_over():\n",
    "            game.determine_winner()\n",
    "            done = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    play_game()\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Handling Multiple Players*\n",
    "\n",
    "If you plan to extend this to more than two players, you'll need to adjust the code accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### *Final Notes*\n",
    "\n",
    "- *Model Evaluation Mode*: Ensure that you set your models to evaluation mode using model.eval() to disable dropout and batch normalization during inference.\n",
    "\n",
    "- *Error Handling*: Add error handling for invalid inputs and edge cases.\n",
    "\n",
    "- *User Interface*: For a better user experience, consider creating a simple GUI or using a library like curses for a terminal-based interface.\n",
    "\n",
    "- *Testing*: Thoroughly test your game loop to ensure that the game logic works as expected.\n",
    "\n",
    "---\n",
    "\n",
    "### *Example: Saving and Loading Models*\n",
    "\n",
    "*Saving Models:*\n",
    "\n",
    "python\n",
    "# After training\n",
    "torch.save({\n",
    "    'policy_net_state_dict': policy_net.state_dict(),\n",
    "    'value_net_state_dict': value_net.state_dict(),\n",
    "}, 'poker_ai_model.pth')\n",
    "\n",
    "\n",
    "*Loading Models:*\n",
    "\n",
    "python\n",
    "# In your game script\n",
    "checkpoint = torch.load('poker_ai_model.pth')\n",
    "policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "value_net.load_state_dict(checkpoint['value_net_state_dict'])\n",
    "policy_net.eval()\n",
    "value_net.eval()\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Conclusion*\n",
    "\n",
    "By following these steps, you can set up a game where you play against your trained AI. Adjust the code to fit your specific needs and consider adding more features to enhance the gameplay experience.\n",
    "\n",
    "If you have any questions or need further assistance with specific parts of the implementation, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It seems you're encountering two main issues:\n",
    "\n",
    "1. *Input Not Recognized*: When you input actions like \"FOLD\", \"fold\", or \"0\", the code outputs \"Invalid action name. Please try again.\"\n",
    "\n",
    "2. *Desired Functionality*: You want to provide specific inputs to the AI—such as your hand, community cards, and opponent's actions—and have the AI suggest plays for you, rather than playing a full game where the AI assumes random cards.\n",
    "\n",
    "Let's address each issue step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### *Issue 1: Input Not Recognized*\n",
    "\n",
    "#### *Problem Analysis*\n",
    "\n",
    "The error occurs because there are multiple definitions of the Action enum in your code, leading to inconsistencies. Specifically:\n",
    "\n",
    "- You define Action in both modules.py and Cell 2 of your 5.ipynb.\n",
    "- When you compare actions or try to create an Action from user input, the code may be using different Action definitions, causing comparisons to fail and exceptions to be raised.\n",
    "\n",
    "#### *Solution*\n",
    "\n",
    "1. *Remove Redundant Definitions*: Remove the Action enum definition from Cell 2 in your 5.ipynb.\n",
    "\n",
    "2. **Import Action from modules**: Ensure that you import Action from modules.py wherever it's needed.\n",
    "\n",
    "3. **Update Imports in 5.ipynb**\n",
    "\n",
    "   In Cell 2, modify your code as follows:\n",
    "\n",
    "   python\n",
    "   import torch\n",
    "\n",
    "   # Remove this definition\n",
    "   # class Action(Enum):\n",
    "   #     FOLD = 0\n",
    "   #     CHECK = 1\n",
    "   #     CALL = 2\n",
    "   #     RAISE = 3\n",
    "   #     BET = 4\n",
    "\n",
    "   # Import Action and MAX_FEATURE_LENGTH from modules\n",
    "   from modules import Action, MAX_FEATURE_LENGTH\n",
    "\n",
    "   # Initialize neural networks\n",
    "   from modules import PolicyNetwork, ValueNetwork\n",
    "   policy_net = PolicyNetwork(input_size=MAX_FEATURE_LENGTH, action_space=len(Action))\n",
    "   value_net = ValueNetwork(input_size=MAX_FEATURE_LENGTH)\n",
    "\n",
    "   # Load the saved models\n",
    "   policy_net.load_state_dict(torch.load('policy_net.pth'))\n",
    "   value_net.load_state_dict(torch.load('value_net.pth'))\n",
    "\n",
    "   # Set networks to evaluation mode\n",
    "   policy_net.eval()\n",
    "   value_net.eval()\n",
    "   \n",
    "\n",
    "4. *Ensure Consistency in All Cells*\n",
    "\n",
    "   - In any cell where you use Action, import it from modules:\n",
    "\n",
    "     python\n",
    "     from modules import Action\n",
    "     \n",
    "\n",
    "5. **Modify human_decide_action Function**\n",
    "\n",
    "   To improve user input handling and allow the user to input either the action name or index, modify Cell 6 as follows:\n",
    "\n",
    "   python\n",
    "   def human_decide_action(human_player, game):\n",
    "       valid_actions = game.get_available_actions(human_player)\n",
    "       print(f\"Your hand: {human_player.hand}\")\n",
    "       print(f\"Community cards: {game.board}\")\n",
    "       print(f\"Pot size: {game.pot}\")\n",
    "       print(f\"Your stack: {human_player.stack}\")\n",
    "       print(f\"Current bet: {game.current_bet}\")\n",
    "       # Map valid actions to indices\n",
    "       valid_actions_list = list(enumerate([action.name for action in valid_actions]))\n",
    "       print(\"Valid actions:\")\n",
    "       for idx, action_name in valid_actions_list:\n",
    "           print(f\"{idx}: {action_name}\")\n",
    "       \n",
    "       action = None\n",
    "       while action not in valid_actions:\n",
    "           action_input = input(\"Enter your action (number or name): \").upper()\n",
    "           try:\n",
    "               # Try to parse as integer index\n",
    "               if action_input.isdigit():\n",
    "                   idx = int(action_input)\n",
    "                   if 0 <= idx < len(valid_actions):\n",
    "                       action = valid_actions[idx]\n",
    "                   else:\n",
    "                       print(\"Invalid action index. Please choose from the valid actions.\")\n",
    "               else:\n",
    "                   # Try to parse as action name\n",
    "                   action = Action[action_input]\n",
    "                   if action not in valid_actions:\n",
    "                       print(\"Invalid action. Please choose from the valid actions.\")\n",
    "           except (KeyError, ValueError):\n",
    "               print(\"Invalid action name or index. Please try again.\")\n",
    "       \n",
    "       # Determine raise amount if necessary\n",
    "       if action in [Action.BET, Action.RAISE]:\n",
    "           raise_amount = float(input(\"Enter raise amount: \"))\n",
    "           game.execute_action(human_player, action, raise_amount=raise_amount)\n",
    "       else:\n",
    "           game.execute_action(human_player, action)\n",
    "       \n",
    "       return action\n",
    "   \n",
    "\n",
    "   *Explanation*:\n",
    "\n",
    "   - This function now accepts either the action name (e.g., \"FOLD\") or the action index (e.g., \"0\").\n",
    "   - It prints the valid actions with their indices for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "### *Issue 2: Desired Functionality*\n",
    "\n",
    "You want to provide specific inputs to the AI—such as your hand, community cards, and opponent's actions—and have the AI suggest plays for you. Essentially, you want to test the AI in specific scenarios rather than playing a full game.\n",
    "\n",
    "#### *Solution Overview*\n",
    "\n",
    "We'll create a function that allows you to input:\n",
    "\n",
    "- *Your Hand*: The cards you're holding.\n",
    "- *Community Cards*: The cards on the table.\n",
    "- *Observed Actions*: Actions taken by the opponent so far.\n",
    "- *Pot Size*: Current pot size.\n",
    "- *Your Stack and Current Bet*: Your remaining chips and current bet.\n",
    "- *Valid Actions*: Actions you can take in the current state.\n",
    "\n",
    "Based on these inputs, the AI will suggest an action.\n",
    "\n",
    "#### *Implementing the Solution*\n",
    "\n",
    "1. **Modify extract_features to Include Hand**\n",
    "\n",
    "   Update the extract_features function in modules.py to include the player's hand:\n",
    "\n",
    "   python\n",
    "   def extract_features(belief_state):\n",
    "       features = []\n",
    "       \n",
    "       # Encode observed actions\n",
    "       action_encoding = {\n",
    "           Action.FOLD: 0,\n",
    "           Action.CHECK: 1,\n",
    "           Action.CALL: 2,\n",
    "           Action.RAISE: 3,\n",
    "           Action.BET: 4\n",
    "       }\n",
    "       max_history_length = 12\n",
    "       action_features = [action_encoding[action] for action in belief_state.observed_actions[-max_history_length:]]\n",
    "       action_features += [0] * (max_history_length - len(action_features))\n",
    "       features.extend(action_features)\n",
    "       \n",
    "       # Encode public cards\n",
    "       rank_encoding = {r: i for i, r in enumerate(range(2, 15), start=1)}\n",
    "       suit_encoding = {'H': 0, 'D': 1, 'C': 2, 'S': 3}\n",
    "       max_board_cards = 5\n",
    "       board_features = []\n",
    "       for rank, suit in belief_state.public_cards:\n",
    "           rank_feature = rank_encoding.get(rank, 0)\n",
    "           suit_feature = suit_encoding.get(suit, 0)\n",
    "           board_features.extend([rank_feature, suit_feature])\n",
    "       while len(board_features) < max_board_cards * 2:\n",
    "           board_features.extend([0, 0])\n",
    "       features.extend(board_features)\n",
    "       \n",
    "       # Encode private cards (player's hand)\n",
    "       hand_features = []\n",
    "       if belief_state.private_cards is not None:\n",
    "           for rank, suit in belief_state.private_cards:\n",
    "               rank_feature = rank_encoding.get(rank, 0)\n",
    "               suit_feature = suit_encoding.get(suit, 0)\n",
    "               hand_features.extend([rank_feature, suit_feature])\n",
    "       else:\n",
    "           hand_features.extend([0, 0, 0, 0])  # Assuming 2 hole cards\n",
    "       features.extend(hand_features)\n",
    "       \n",
    "       # Encode pot size\n",
    "       features.append(belief_state.pot_size / 1000)  # Normalize pot size\n",
    "       \n",
    "       # Ensure total feature length matches MAX_FEATURE_LENGTH\n",
    "       features = features[:MAX_FEATURE_LENGTH] + [0] * max(0, MAX_FEATURE_LENGTH - len(features))\n",
    "       \n",
    "       # Convert to tensor\n",
    "       features = torch.tensor(features, dtype=torch.float32)\n",
    "       return features\n",
    "   \n",
    "\n",
    "2. *Create a Function to Suggest Actions*\n",
    "\n",
    "   In 5.ipynb, create a new function ai_suggest_action:\n",
    "\n",
    "   python\n",
    "   def ai_suggest_action(\n",
    "       player_hand,\n",
    "       community_cards,\n",
    "       observed_actions,\n",
    "       pot_size,\n",
    "       player_stack,\n",
    "       player_current_bet,\n",
    "       current_bet,\n",
    "       valid_actions\n",
    "   ):\n",
    "       # Create a belief state\n",
    "       belief_state = BeliefState(\n",
    "           observed_actions=observed_actions,\n",
    "           public_cards=community_cards,\n",
    "           pot_size=pot_size\n",
    "       )\n",
    "       belief_state.private_cards = player_hand\n",
    "       \n",
    "       # Create a dummy player object\n",
    "       player = Player(id=0, stack=player_stack)\n",
    "       player.hand = player_hand\n",
    "       player.current_bet = player_current_bet\n",
    "       player.folded = False\n",
    "       \n",
    "       # Extract features\n",
    "       features = extract_features(belief_state)\n",
    "       if features.dim() == 1:\n",
    "           features = features.unsqueeze(0)\n",
    "       \n",
    "       # Get action probabilities\n",
    "       with torch.no_grad():\n",
    "           action_probs = policy_net(features)\n",
    "       action_probs = action_probs.squeeze(0)\n",
    "       \n",
    "       # Choose an action\n",
    "       action = sample_action(action_probs, valid_actions)\n",
    "       return action\n",
    "   \n",
    "\n",
    "3. *Use the Function with Custom Inputs*\n",
    "\n",
    "   In a new cell, input your desired parameters:\n",
    "\n",
    "   python\n",
    "   # Sample inputs\n",
    "   from modules import sample_action, extract_features, determine_raise_amount, Action\n",
    "\n",
    "   # Input your hand\n",
    "   player_hand = [(14, 'H'), (14, 'D')]  # For example, Ace of Hearts and Ace of Diamonds\n",
    "\n",
    "   # Input community cards\n",
    "   community_cards = [(10, 'C'), (11, 'D'), (12, 'S'), (13, 'H'), (9, 'C')]  # For example\n",
    "\n",
    "   # Input observed actions (opponent's actions)\n",
    "   observed_actions = [Action.CALL, Action.RAISE]\n",
    "\n",
    "   # Input pot size, your stack, and current bets\n",
    "   pot_size = 500\n",
    "   player_stack = 1000\n",
    "   player_current_bet = 0\n",
    "   current_bet = 100  # The amount you need to call\n",
    "\n",
    "   # Define valid actions in the current state\n",
    "   valid_actions = [Action.FOLD, Action.CALL, Action.RAISE]\n",
    "\n",
    "   # Get the AI's suggested action\n",
    "   suggested_action = ai_suggest_action(\n",
    "       player_hand=player_hand,\n",
    "       community_cards=community_cards,\n",
    "       observed_actions=observed_actions,\n",
    "       pot_size=pot_size,\n",
    "       player_stack=player_stack,\n",
    "       player_current_bet=player_current_bet,\n",
    "       current_bet=current_bet,\n",
    "       valid_actions=valid_actions\n",
    "   )\n",
    "\n",
    "   print(f\"AI suggests action: {suggested_action.name}\")\n",
    "   \n",
    "\n",
    "   *Explanation*:\n",
    "\n",
    "   - *Player Hand*: Your cards.\n",
    "   - *Community Cards*: Cards on the table.\n",
    "   - *Observed Actions*: Actions taken by the opponent.\n",
    "   - *Pot Size*: Current total pot.\n",
    "   - *Player Stack and Current Bet*: Your remaining chips and the amount you've currently bet.\n",
    "   - *Current Bet*: The amount you need to call to stay in the hand.\n",
    "   - *Valid Actions*: Actions you can take in this situation.\n",
    "\n",
    "4. **Adjust MAX_FEATURE_LENGTH If Necessary**\n",
    "\n",
    "   Ensure that MAX_FEATURE_LENGTH in both modules.py and 5.ipynb matches the length of your feature vector after including the player's hand.\n",
    "\n",
    "5. **Update BeliefState in modules.py**\n",
    "\n",
    "   Ensure that the BeliefState class in modules.py includes private_cards:\n",
    "\n",
    "   python\n",
    "   class BeliefState:\n",
    "       def __init__(self, observed_actions, public_cards, pot_size=0):\n",
    "           self.observed_actions = observed_actions\n",
    "           self.public_cards = public_cards\n",
    "           self.private_cards = None\n",
    "           self.pot_size = pot_size\n",
    "\n",
    "       def update(self, action, new_public_cards=None, pot_size=None):\n",
    "           self.observed_actions.append(action)\n",
    "           if new_public_cards is not None:\n",
    "               self.public_cards = new_public_cards\n",
    "           if pot_size is not None:\n",
    "               self.pot_size = pot_size\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "### *Summary*\n",
    "\n",
    "- *Fix Input Recognition*: Ensure that you use the same Action enum throughout your code by importing it from modules.py and removing duplicate definitions.\n",
    "\n",
    "- *Implement Desired Functionality*: Create a function that allows you to input specific game parameters and get the AI's suggested action based on those inputs.\n",
    "\n",
    "- *Adjust Feature Extraction*: Modify extract_features to include your hand and any additional information you want the AI to consider.\n",
    "\n",
    "---\n",
    "\n",
    "### *Additional Tips*\n",
    "\n",
    "- *Test with Different Scenarios*: Try different hand combinations, community cards, and observed actions to see how the AI responds.\n",
    "\n",
    "- *Validate Inputs*: Ensure that the inputs you provide are valid within the game's rules (e.g., valid card ranks and suits).\n",
    "\n",
    "- *Expand Valid Actions*: Adjust the valid_actions list based on the game's state (e.g., you can't CHECK if there's a bet you need to call).\n",
    "\n",
    "---\n",
    "\n",
    "### *Conclusion*\n",
    "\n",
    "By making these adjustments, you'll be able to provide specific inputs to the AI and have it suggest actions based on the game state you define. This setup allows you to test the AI's decision-making in various scenarios and better understand its strategy.\n",
    "\n",
    "If you have any further questions or need assistance with specific parts of the implementation, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I understand that you're experiencing a few issues:\n",
    "\n",
    "1. *Understanding the output of Cell 9*.\n",
    "2. *Wanting to input your own hand rather than having it randomly generated*.\n",
    "3. **An error occurring before the game finishes: KeyError: 'D'**.\n",
    "\n",
    "Let's address each of these concerns step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### *1. Understanding the Output of Cell 9*\n",
    "\n",
    "*Cell 9 Output:*\n",
    "\n",
    "AI suggests action: FOLD\n",
    "\n",
    "\n",
    "*Explanation:*\n",
    "\n",
    "- *Purpose of Cell 9*: This cell is designed to test the ai_suggest_action function by providing it with specific inputs. It simulates a scenario where you provide the AI with certain game parameters, and it outputs the suggested action based on those inputs.\n",
    "\n",
    "- *No Interaction Needed*: You don't need to interact with Cell 9. It's not expecting any input from you during execution. It simply runs the function with the hardcoded sample inputs and prints the AI's suggested action.\n",
    "\n",
    "- *Result Interpretation: The AI, given the specific inputs (e.g., your hand, community cards, observed actions, pot size, etc.), has determined that the best action is to **FOLD*. This is based on the model's current state and training.\n",
    "\n",
    "*Why Did the AI Suggest \"FOLD\"?*\n",
    "\n",
    "- The AI's suggestion is based on its learned policy. It's possible that, given the inputs, it evaluated the situation as unfavorable and recommended folding.\n",
    "\n",
    "- Since the AI is trained based on your code and training data, its decisions are influenced by that training. If you believe the AI should have suggested a different action, you may need to revisit the training data or model parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### *2. Inputting Your Own Hand Instead of Randomly Generating It*\n",
    "\n",
    "*Current Behavior:*\n",
    "\n",
    "- In *Cell 10*, the game is set up to deal random hands to players using the game.deal_hole_cards() method.\n",
    "\n",
    "*Your Requirement:*\n",
    "\n",
    "- You want to specify your own hand rather than having it randomly assigned.\n",
    "\n",
    "*Solution:*\n",
    "\n",
    "To allow you to input your own hand, we need to modify the code to set the player's hand manually.\n",
    "\n",
    "#### *Steps to Modify the Code:*\n",
    "\n",
    "1. **Remove or Comment Out the deal_hole_cards() Method Call**\n",
    "\n",
    "   In *Cell 3*, comment out or remove the line:\n",
    "\n",
    "   python\n",
    "   game.deal_hole_cards()\n",
    "   \n",
    "\n",
    "   We will assign the hands manually instead.\n",
    "\n",
    "2. *Assign Hands Manually*\n",
    "\n",
    "   After initializing the game, set the hand attribute of each player to your desired cards.\n",
    "\n",
    "   python\n",
    "   # Input your desired hands\n",
    "   ai_player.hand = [(14, 'H'), (14, 'D')]  # AI player's hand (e.g., Ace of Hearts and Ace of Diamonds)\n",
    "   human_player.hand = [(12, 'D'), (8, 'S')]  # Your hand (e.g., Queen of Diamonds and 8 of Spades)\n",
    "   \n",
    "\n",
    "   *Note:* Ensure that the cards you assign are not duplicated and are removed from the deck if the game logic depends on the deck.\n",
    "\n",
    "3. *Update the Deck to Remove Dealt Cards*\n",
    "\n",
    "   Since we're manually assigning cards, we need to make sure these cards are removed from the deck to avoid duplicates.\n",
    "\n",
    "   python\n",
    "   # Remove assigned cards from the deck\n",
    "   assigned_cards = ai_player.hand + human_player.hand\n",
    "   for card in assigned_cards:\n",
    "       if card in game.deck:\n",
    "           game.deck.remove(card)\n",
    "   \n",
    "\n",
    "4. *Initialize the Belief States with Private Cards*\n",
    "\n",
    "   When creating the BeliefState for the AI, include the AI's hand:\n",
    "\n",
    "   python\n",
    "   ai_belief_state = BeliefState(\n",
    "       observed_actions=[],\n",
    "       public_cards=game.board,\n",
    "       pot_size=game.pot\n",
    "   )\n",
    "   ai_belief_state.private_cards = ai_player.hand\n",
    "   \n",
    "\n",
    "5. *Adjust the Feature Extraction if Necessary*\n",
    "\n",
    "   Ensure that extract_features includes the AI's private cards, which you've already updated.\n",
    "\n",
    "6. *Proceed with the Game Loop*\n",
    "\n",
    "   Now, when you run *Cell 10*, the game will use the hands you've assigned rather than random ones.\n",
    "\n",
    "#### *Updated Code Example for Cell 3:*\n",
    "\n",
    "python\n",
    "# Initialize players\n",
    "from modules import Player\n",
    "ai_player = Player(id=0, stack=1000)\n",
    "human_player = Player(id=1, stack=1000)\n",
    "players = [ai_player, human_player]\n",
    "\n",
    "# Initialize the game\n",
    "from modules import TexasHoldEm\n",
    "game = TexasHoldEm(players)\n",
    "# game.deal_hole_cards()  # Commented out since we're assigning hands manually\n",
    "game.post_blinds()\n",
    "\n",
    "# Assign hands manually\n",
    "ai_player.hand = [(14, 'H'), (14, 'D')]  # AI player's hand\n",
    "human_player.hand = [(12, 'D'), (8, 'S')]  # Human player's hand\n",
    "\n",
    "# Remove assigned cards from the deck\n",
    "assigned_cards = ai_player.hand + human_player.hand\n",
    "for card in assigned_cards:\n",
    "    if card in game.deck:\n",
    "        game.deck.remove(card)\n",
    "\n",
    "\n",
    "#### *Update Cell 4:*\n",
    "\n",
    "Include the AI's private cards in the belief state:\n",
    "\n",
    "python\n",
    "from modules import BeliefState\n",
    "ai_belief_state = BeliefState(\n",
    "    observed_actions=[],\n",
    "    public_cards=game.board,\n",
    "    pot_size=game.pot\n",
    ")\n",
    "ai_belief_state.private_cards = ai_player.hand\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Error Occurring Before the Game Finishes: KeyError: 'D'**\n",
    "\n",
    "*Error Message:*\n",
    "\n",
    "\n",
    "KeyError: 'D'\n",
    "\n",
    "\n",
    "*Traceback:*\n",
    "\n",
    "\n",
    "File c:\\...treys\\card.py:70, in Card.new(string)\n",
    "     suit_int = Card.CHAR_SUIT_TO_INT_SUIT[suit_char]\n",
    "\n",
    "\n",
    "*Explanation:*\n",
    "\n",
    "- The error occurs when the game tries to determine the winner and uses the treys library to evaluate the hands.\n",
    "\n",
    "- The treys library expects suit characters to be one of 's', 'h', 'd', 'c' (lowercase letters).\n",
    "\n",
    "- In your code, suits are represented with uppercase letters ('H', 'D', 'C', 'S'), which causes a KeyError when the treys library attempts to look up the suit.\n",
    "\n",
    "*Solution:*\n",
    "\n",
    "We need to ensure that the suit characters are in lowercase and match the expected values in the treys library.\n",
    "\n",
    "#### *Steps to Fix the Error:*\n",
    "\n",
    "1. **Modify the rank_to_str Function to Return Lowercase Suit Characters**\n",
    "\n",
    "   Update the rank_to_str function in modules.py to return lowercase suits.\n",
    "\n",
    "   **Current rank_to_str Function:**\n",
    "\n",
    "   python\n",
    "   def rank_to_str(self, rank):\n",
    "       if rank == 14:\n",
    "           return 'A'\n",
    "       elif rank == 13:\n",
    "           return 'K'\n",
    "       elif rank == 12:\n",
    "           return 'Q'\n",
    "       elif rank == 11:\n",
    "           return 'J'\n",
    "       elif rank == 10:\n",
    "           return 'T'\n",
    "       else:\n",
    "           return str(rank)\n",
    "   \n",
    "\n",
    "   *Modification:*\n",
    "\n",
    "   Since the issue is with the suits, we need a function to convert suits to lowercase and to the expected characters.\n",
    "\n",
    "2. **Create a suit_to_str Function**\n",
    "\n",
    "   Add a function to convert the suit to the appropriate character.\n",
    "\n",
    "   python\n",
    "   def suit_to_str(self, suit):\n",
    "       suit_mapping = {\n",
    "           'H': 'h',  # Hearts\n",
    "           'D': 'd',  # Diamonds\n",
    "           'C': 'c',  # Clubs\n",
    "           'S': 's'   # Spades\n",
    "       }\n",
    "       return suit_mapping.get(suit.upper(), 'Invalid')\n",
    "   \n",
    "\n",
    "3. **Update the determine_winner Method**\n",
    "\n",
    "   In the determine_winner method, update the code where you create the cards for the treys library.\n",
    "\n",
    "   *Original Code:*\n",
    "\n",
    "   python\n",
    "   hand = [Card.new(f\"{self.rank_to_str(rank)}{suit}\") for rank, suit in player.hand]\n",
    "   board = [Card.new(f\"{self.rank_to_str(rank)}{suit}\") for rank, suit in self.board]\n",
    "   \n",
    "\n",
    "   *Modified Code:*\n",
    "\n",
    "   python\n",
    "   hand = [Card.new(f\"{self.rank_to_str(rank)}{self.suit_to_str(suit)}\") for rank, suit in player.hand]\n",
    "   board = [Card.new(f\"{self.rank_to_str(rank)}{self.suit_to_str(suit)}\") for rank, suit in self.board]\n",
    "   \n",
    "\n",
    "4. *Ensure Ranks Are Correctly Converted*\n",
    "\n",
    "   Double-check that rank_to_str correctly converts numerical ranks to the expected characters for the treys library.\n",
    "\n",
    "5. *Handle Invalid Suits*\n",
    "\n",
    "   In the suit_to_str function, if the suit is invalid, you can raise an error or handle it appropriately.\n",
    "\n",
    "#### **Updated Code Snippet in modules.py:**\n",
    "\n",
    "python\n",
    "def rank_to_str(self, rank):\n",
    "    if rank == 14:\n",
    "        return 'A'\n",
    "    elif rank == 13:\n",
    "        return 'K'\n",
    "    elif rank == 12:\n",
    "        return 'Q'\n",
    "    elif rank == 11:\n",
    "        return 'J'\n",
    "    elif rank == 10:\n",
    "        return 'T'\n",
    "    else:\n",
    "        return str(rank)\n",
    "\n",
    "def suit_to_str(self, suit):\n",
    "    suit_mapping = {\n",
    "        'H': 'h',  # Hearts\n",
    "        'D': 'd',  # Diamonds\n",
    "        'C': 'c',  # Clubs\n",
    "        'S': 's'   # Spades\n",
    "    }\n",
    "    return suit_mapping.get(suit.upper(), 'Invalid')\n",
    "\n",
    "def determine_winner(self):\n",
    "    # If only one player remains\n",
    "    active_players = [p for p in self.players if not p.folded]\n",
    "    if len(active_players) == 1:\n",
    "        winner = active_players[0]\n",
    "        winner.stack += self.pot\n",
    "        print(f\"Player {winner.id} wins the pot of {self.pot} by default.\")\n",
    "        self.pot = 0\n",
    "        return\n",
    "\n",
    "    # Showdown: compare hands\n",
    "    from treys import Evaluator, Card\n",
    "    evaluator = Evaluator()\n",
    "    best_rank = None\n",
    "    winners = []\n",
    "    for player in active_players:\n",
    "        hand = [Card.new(f\"{self.rank_to_str(rank)}{self.suit_to_str(suit)}\") for rank, suit in player.hand]\n",
    "        board = [Card.new(f\"{self.rank_to_str(rank)}{self.suit_to_str(suit)}\") for rank, suit in self.board]\n",
    "        rank = evaluator.evaluate(board, hand)\n",
    "        if best_rank is None or rank < best_rank:\n",
    "            best_rank = rank\n",
    "            winners = [player]\n",
    "        elif rank == best_rank:\n",
    "            winners.append(player)\n",
    "    # Split the pot among winners\n",
    "    split_pot = self.pot / len(winners)\n",
    "    for winner in winners:\n",
    "        winner.stack += split_pot\n",
    "        print(f\"Player {winner.id} wins {split_pot} from the pot.\")\n",
    "    self.pot = 0\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### *Additional Notes*\n",
    "\n",
    "#### *Regarding Cell 10*\n",
    "\n",
    "- *Purpose of Cell 10*: It runs the game loop, allowing you to play a game against the AI.\n",
    "\n",
    "- *Interactions*: You are expected to input your actions when prompted.\n",
    "\n",
    "- *Valid Actions Input*: With the updated human_decide_action function, you can enter either the action name (e.g., \"FOLD\") or the index number (e.g., \"0\").\n",
    "\n",
    "#### *Testing the Updated Code*\n",
    "\n",
    "1. **Save Changes to modules.py**: Make sure you save the updated modules.py file after making the changes.\n",
    "\n",
    "2. *Restart Your Notebook Kernel*: Restart the kernel in your Jupyter notebook to ensure that the updated modules.py is loaded.\n",
    "\n",
    "3. *Reload the Module*:\n",
    "\n",
    "   In *Cell 1* (or a new cell), add:\n",
    "\n",
    "   python\n",
    "   import importlib\n",
    "   import modules\n",
    "   importlib.reload(modules)\n",
    "   \n",
    "\n",
    "4. *Re-import Necessary Classes and Functions*:\n",
    "\n",
    "   Update your imports to ensure you're using the latest versions:\n",
    "\n",
    "   python\n",
    "   from modules import (\n",
    "       Action,\n",
    "       MAX_FEATURE_LENGTH,\n",
    "       PolicyNetwork,\n",
    "       ValueNetwork,\n",
    "       Player,\n",
    "       TexasHoldEm,\n",
    "       BeliefState,\n",
    "       extract_features,\n",
    "       sample_action,\n",
    "       determine_raise_amount\n",
    "   )\n",
    "   \n",
    "\n",
    "5. *Run All Cells in Order*: Execute each cell sequentially after making the necessary changes.\n",
    "\n",
    "---\n",
    "\n",
    "### *Summary*\n",
    "\n",
    "- *Cell 9 Output*: The AI suggested \"FOLD\" based on the inputs provided. No interaction is required in this cell.\n",
    "\n",
    "- *Inputting Your Own Hand*: Modify the code to assign your desired hand to the player and remove those cards from the deck to prevent duplicates.\n",
    "\n",
    "- *Error Before Game Finishes*: The KeyError: 'D' occurs because the treys library expects lowercase suit characters. Update the code to convert suit characters to the expected format.\n",
    "\n",
    "---\n",
    "\n",
    "### *Final Thoughts*\n",
    "\n",
    "By making these changes, you should be able to:\n",
    "\n",
    "- Provide your own hand and play against the AI with known cards.\n",
    "\n",
    "- Avoid errors related to suit characters when determining the winner.\n",
    "\n",
    "- Proceed with the game until it reaches a conclusion.\n",
    "\n",
    "---\n",
    "\n",
    "If you encounter any further issues or have additional questions, please feel free to ask, and I'll be happy to assist you further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! To modify your existing Jupyter Notebook to continuously accept inputs at each turn and have the AI compute and suggest actions using your neural network model, follow the steps below. This modification will enable the AI to operate in an infinite loop, processing your inputs and providing actions until you decide to exit.\n",
    "\n",
    "### Overview of Changes\n",
    "\n",
    "1. **Create an Infinite Loop for AI Decision-Making:**\n",
    "   - This loop will continuously prompt you for the necessary inputs at each turn.\n",
    "   - It will utilize the `ai_suggest_action` function to compute the preferred action based on your inputs.\n",
    "   - The loop will run indefinitely until you choose to exit.\n",
    "\n",
    "2. **Handle Input Validation:**\n",
    "   - Ensure that all inputs are correctly formatted to prevent runtime errors.\n",
    "   - Provide clear prompts and feedback for incorrect inputs.\n",
    "\n",
    "3. **Address the Existing Error:**\n",
    "   - The `KeyError: 'D'` suggests an issue with card suit encoding. Ensure that all suits are correctly mapped in the `Card.new` function.\n",
    "\n",
    "### Step-by-Step Implementation\n",
    "\n",
    "#### **1. Modify or Add a New Cell for the Infinite Loop**\n",
    "\n",
    "Add a new cell (e.g., **Cell 11**) at the end of your notebook with the following code. This cell will handle the continuous input-output loop for the AI's decision-making.\n",
    "\n",
    "```python\n",
    "# Cell 11: Infinite Loop for AI Decision-Making\n",
    "\n",
    "import sys\n",
    "\n",
    "def input_hand(prompt):\n",
    "    \"\"\"\n",
    "    Prompts the user to input their hand cards.\n",
    "    Expected format: '14H 14D' for Ace of Hearts and Ace of Diamonds.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        hand_input = input(prompt)\n",
    "        try:\n",
    "            cards = hand_input.strip().split()\n",
    "            if len(cards) != 2:\n",
    "                print(\"Please enter exactly two cards, e.g., 14H 14D\")\n",
    "                continue\n",
    "            hand = []\n",
    "            for card in cards:\n",
    "                rank = int(card[:-1])\n",
    "                suit = card[-1].upper()\n",
    "                if suit not in ['H', 'D', 'C', 'S']:\n",
    "                    raise ValueError\n",
    "                hand.append((rank, suit))\n",
    "            return hand\n",
    "        except ValueError:\n",
    "            print(\"Invalid card format. Please enter as RankSuit, e.g., 14H for Ace of Hearts.\")\n",
    "\n",
    "def input_community_cards():\n",
    "    \"\"\"\n",
    "    Prompts the user to input community cards.\n",
    "    Expected format: '10C 11D 12S 13H 9C'\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        community_input = input(\"Enter community cards separated by space (e.g., 10C 11D 12S 13H 9C), or press Enter if none: \")\n",
    "        if not community_input.strip():\n",
    "            return []\n",
    "        try:\n",
    "            cards = community_input.strip().split()\n",
    "            community = []\n",
    "            for card in cards:\n",
    "                rank = int(card[:-1])\n",
    "                suit = card[-1].upper()\n",
    "                if suit not in ['H', 'D', 'C', 'S']:\n",
    "                    raise ValueError\n",
    "                community.append((rank, suit))\n",
    "            if len(community) > 5:\n",
    "                print(\"Maximum of 5 community cards allowed.\")\n",
    "                continue\n",
    "            return community\n",
    "        except ValueError:\n",
    "            print(\"Invalid card format. Please enter as RankSuit, e.g., 10C for Ten of Clubs.\")\n",
    "\n",
    "def input_actions(prompt):\n",
    "    \"\"\"\n",
    "    Prompts the user to input a list of valid actions.\n",
    "    Expected format: 'FOLD CALL RAISE'\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        actions_input = input(prompt)\n",
    "        try:\n",
    "            action_names = actions_input.strip().upper().split()\n",
    "            actions = [Action[action] for action in action_names]\n",
    "            if not actions:\n",
    "                print(\"Please enter at least one action.\")\n",
    "                continue\n",
    "            return actions\n",
    "        except KeyError:\n",
    "            print(\"Invalid action name. Valid actions are: FOLD, CHECK, CALL, RAISE, BET.\")\n",
    "\n",
    "def input_observed_actions():\n",
    "    \"\"\"\n",
    "    Prompts the user to input a list of observed opponent actions.\n",
    "    Expected format: 'CALL RAISE'\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        actions_input = input(\"Enter observed opponent actions separated by space (e.g., CALL RAISE), or press Enter if none: \")\n",
    "        if not actions_input.strip():\n",
    "            return []\n",
    "        try:\n",
    "            action_names = actions_input.strip().upper().split()\n",
    "            actions = [Action[action] for action in action_names]\n",
    "            return actions\n",
    "        except KeyError:\n",
    "            print(\"Invalid action name. Valid actions are: FOLD, CHECK, CALL, RAISE, BET.\")\n",
    "\n",
    "def input_float(prompt):\n",
    "    \"\"\"\n",
    "    Prompts the user to input a floating-point number.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        val = input(prompt)\n",
    "        try:\n",
    "            return float(val)\n",
    "        except ValueError:\n",
    "            print(\"Please enter a valid number.\")\n",
    "\n",
    "def main_loop():\n",
    "    \"\"\"\n",
    "    Main loop that continuously takes inputs and outputs AI's suggested action.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        print(\"\\n--- New Turn ---\")\n",
    "        try:\n",
    "            player_stack = input_float(\"Enter your stack: \")\n",
    "            opponent_actions = input_observed_actions()\n",
    "            player_hand = input_hand(\"Enter your hand (e.g., 14H 14D): \")\n",
    "            community_cards = input_community_cards()\n",
    "            valid_actions = input_actions(\"Enter valid actions separated by space (e.g., FOLD CALL RAISE): \")\n",
    "            current_bet = input_float(\"Enter current bet: \")\n",
    "            pot_size = input_float(\"Enter pot size: \")\n",
    "            player_current_bet = input_float(\"Enter your current bet: \")\n",
    "\n",
    "            # Get the AI's suggested action\n",
    "            suggested_action = ai_suggest_action(\n",
    "                player_hand=player_hand,\n",
    "                community_cards=community_cards,\n",
    "                observed_actions=opponent_actions,\n",
    "                pot_size=pot_size,\n",
    "                player_stack=player_stack,\n",
    "                player_current_bet=player_current_bet,\n",
    "                current_bet=current_bet,\n",
    "                valid_actions=valid_actions\n",
    "            )\n",
    "            \n",
    "            print(f\"AI suggests action: {suggested_action.name}\")\n",
    "\n",
    "            # Option to exit\n",
    "            exit_input = input(\"Do you want to exit? (y/n): \").lower()\n",
    "            if exit_input == 'y':\n",
    "                print(\"Exiting...\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            print(\"Restarting the turn...\")\n",
    "\n",
    "# Start the main loop\n",
    "main_loop()\n",
    "```\n",
    "\n",
    "#### **2. Explanation of the Added Code**\n",
    "\n",
    "- **Input Functions:**\n",
    "  - `input_hand(prompt)`: Prompts the user to input their two hole cards in the format `RankSuit` (e.g., `14H` for Ace of Hearts).\n",
    "  - `input_community_cards()`: Allows the user to input up to five community cards or leave it empty if there are none.\n",
    "  - `input_actions(prompt)`: Prompts the user to input a list of valid actions available in the current turn (e.g., `FOLD CALL RAISE`).\n",
    "  - `input_observed_actions()`: Lets the user input a sequence of opponent actions observed so far.\n",
    "  - `input_float(prompt)`: Ensures that numerical inputs like stack size, current bet, and pot size are correctly entered as floating-point numbers.\n",
    "\n",
    "- **`main_loop()` Function:**\n",
    "  - This function encapsulates the infinite loop that handles each turn.\n",
    "  - It sequentially prompts the user for all necessary inputs.\n",
    "  - It calls the `ai_suggest_action` function with the collected inputs to determine the AI's preferred action.\n",
    "  - After displaying the suggested action, it asks the user if they wish to exit the loop.\n",
    "  - The loop handles any unexpected errors gracefully, allowing the user to restart the turn without crashing the entire loop.\n",
    "\n",
    "#### **3. Handling the Existing KeyError**\n",
    "\n",
    "The error you encountered:\n",
    "```\n",
    "KeyError: 'D'\n",
    "```\n",
    "suggests that there's an issue with how the suits are being handled when converting card representations. To resolve this:\n",
    "\n",
    "- **Ensure Consistent Suit Encoding:**\n",
    "  - In your `extract_features` function, suits are encoded as `'H'`, `'D'`, `'C'`, `'S'`.\n",
    "  - Verify that the `treys` library (or any other card handling library you're using) expects suits in the same format.\n",
    "  \n",
    "- **Modify `determine_winner` Function:**\n",
    "  - Update the `determine_winner` method in your `TexasHoldEm` class to ensure that suits are correctly formatted.\n",
    "  - Example modification:\n",
    "    ```python\n",
    "    def determine_winner(self):\n",
    "        # If only one player remains\n",
    "        active_players = [p for p in self.players if not p.folded]\n",
    "        if len(active_players) == 1:\n",
    "            winner = active_players[0]\n",
    "            winner.stack += self.pot\n",
    "            print(f\"Player {winner.id} wins the pot of {self.pot} by default.\")\n",
    "            self.pot = 0\n",
    "            return\n",
    "\n",
    "        # Showdown: compare hands\n",
    "        from treys import Evaluator, Card\n",
    "        evaluator = Evaluator()\n",
    "        best_rank = None\n",
    "        winners = []\n",
    "        for player in active_players:\n",
    "            # Ensure that rank_to_str returns correct strings\n",
    "            hand = [Card.new(f\"{self.rank_to_str(rank)}{suit}\") for rank, suit in player.hand]\n",
    "            board = [Card.new(f\"{self.rank_to_str(rank)}{suit}\") for rank, suit in self.board]\n",
    "            rank = evaluator.evaluate(board, hand)\n",
    "            if best_rank is None or rank < best_rank:\n",
    "                best_rank = rank\n",
    "                winners = [player]\n",
    "            elif rank == best_rank:\n",
    "                winners.append(player)\n",
    "        # Split the pot among winners\n",
    "        split_pot = self.pot / len(winners)\n",
    "        for winner in winners:\n",
    "            winner.stack += split_pot\n",
    "            print(f\"Player {winner.id} wins {split_pot} from the pot.\")\n",
    "        self.pot = 0\n",
    "    ```\n",
    "  - **Check `rank_to_str` Function:**\n",
    "    - Ensure that the `rank_to_str` method correctly converts numerical ranks to their string representations expected by the `treys` library.\n",
    "    - Example:\n",
    "      ```python\n",
    "      def rank_to_str(self, rank):\n",
    "          rank_mapping = {\n",
    "              2: '2', 3: '3', 4: '4', 5: '5',\n",
    "              6: '6', 7: '7', 8: '8', 9: '9',\n",
    "              10: 'T', 11: 'J', 12: 'Q',\n",
    "              13: 'K', 14: 'A'\n",
    "          }\n",
    "          return rank_mapping.get(rank, '2')  # Default to '2' if rank not found\n",
    "      ```\n",
    "\n",
    "#### **4. Running the Modified Notebook**\n",
    "\n",
    "1. **Ensure All Dependencies Are Installed:**\n",
    "   - Make sure that the `treys` library and any other dependencies are correctly installed in your environment.\n",
    "   - You can install `treys` using pip:\n",
    "     ```bash\n",
    "     pip install treys\n",
    "     ```\n",
    "\n",
    "2. **Execute All Cells:**\n",
    "   - Run all the cells in your notebook up to **Cell 11** to initialize the models, players, game, and define necessary functions.\n",
    "\n",
    "3. **Start the Infinite Loop:**\n",
    "   - Run **Cell 11** to start the infinite loop.\n",
    "   - Follow the on-screen prompts to input your stack, actions, hand, community cards, etc.\n",
    "   - The AI will respond with suggested actions based on your inputs.\n",
    "   - To exit the loop, simply enter `'y'` when prompted.\n",
    "\n",
    "#### **5. Example Interaction**\n",
    "\n",
    "```\n",
    "--- New Turn ---\n",
    "Enter your stack: 1000\n",
    "Enter observed opponent actions separated by space (e.g., CALL RAISE), or press Enter if none: CALL RAISE\n",
    "Enter your hand (e.g., 14H 14D): 14H 14D\n",
    "Enter community cards separated by space (e.g., 10C 11D 12S 13H 9C), or press Enter if none: 10C 11D 12S\n",
    "Enter valid actions separated by space (e.g., FOLD CALL RAISE): FOLD CALL RAISE\n",
    "Enter current bet: 100\n",
    "Enter pot size: 500\n",
    "Enter your current bet: 0\n",
    "AI suggests action: RAISE\n",
    "Do you want to exit? (y/n): n\n",
    "\n",
    "--- New Turn ---\n",
    "Enter your stack: 900\n",
    "...\n",
    "```\n",
    "\n",
    "#### **6. Additional Recommendations**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
